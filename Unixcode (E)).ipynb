{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:46:39.710767Z",
     "start_time": "2025-07-16T17:46:38.956748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "gbig_test_df = pd.read_csv(\"big_vul_with_event_trace.csv\")\n",
    "\n",
    "gbig_test_df.info()"
   ],
   "id": "5defb080f2d4b2fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1170 entries, 0 to 1169\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  1170 non-null   object\n",
      " 1   input        1170 non-null   object\n",
      " 2   output       1170 non-null   int64 \n",
      " 3   graph_path   1170 non-null   object\n",
      " 4   event_trace  1170 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 45.8+ KB\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:47:03.696774Z",
     "start_time": "2025-07-16T17:46:41.202414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './cdl_unixcoder_model_final'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "gbig_test_df = gbig_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "gbig_test_df = gbig_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "SEP = \" // LOGIC: \"\n",
    "gbig_test_df['model_input'] =gbig_test_df['func'] + SEP +  gbig_test_df['event_trace']\n",
    "# Convert to HuggingFace Dataset\n",
    "gbig_test_ds = Dataset.from_pandas(gbig_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"event_trace\"]] ,\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "big_test_tok = gbig_test_ds.map(tokenize_fn, batched=True)\n",
    "big_test_tok = big_test_tok.rename_column(\"label\", \"labels\")\n",
    "big_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=big_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on BigVul Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "bc631408484cb948",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1170 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dba82ca0779e432d8a319a074aa7a5b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_9708\\1687572750.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [147/147 00:11]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on BigVul Dataset ===\n",
      "loss        : 2.1946\n",
      "model_preparation_time: 0.0000\n",
      "accuracy    : 0.4863\n",
      "precision   : 0.4864\n",
      "recall      : 0.4906\n",
      "specificity : 0.4821\n",
      "fpr         : 0.5179\n",
      "f1          : 0.4885\n",
      "mcc         : -0.0274\n",
      "kappa       : -0.0274\n",
      "mse         : 0.5137\n",
      "mae         : 0.5137\n",
      "auc         : 0.4883\n",
      "runtime     : 11.4783\n",
      "samples_per_second: 101.9310\n",
      "steps_per_second: 12.8070\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:47:09.913786Z",
     "start_time": "2025-07-16T17:47:09.846751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "gdiv_test_df = pd.read_csv(\"diverse_with_event_trace_graph.csv\")\n",
    "gdiv_test_df.info()"
   ],
   "id": "d6532ed846709702",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1532 entries, 0 to 1531\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   code_snip    1532 non-null   object\n",
      " 1   output       1532 non-null   int64 \n",
      " 2   graph_path   1532 non-null   object\n",
      " 3   event_trace  1532 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 48.0+ KB\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:47:29.135069Z",
     "start_time": "2025-07-16T17:47:11.758260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './cdl_unixcoder_model_final'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "gdiv_test_df = gdiv_test_df.rename(columns={\"code_snip\": \"func\", \"output\": \"label\"})\n",
    "gdiv_test_df = gdiv_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "SEP = \" // LOGIC: \"\n",
    "gdiv_test_df['model_input'] =gdiv_test_df['func'] + SEP +  gdiv_test_df['event_trace']\n",
    "# Convert to HuggingFace Dataset\n",
    "gdiv_test_ds = Dataset.from_pandas(gdiv_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"event_trace\"]] ,\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "div_test_tok = gdiv_test_ds.map(tokenize_fn, batched=True)\n",
    "div_test_tok = div_test_tok.rename_column(\"label\", \"labels\")\n",
    "div_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=div_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on Diverse Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "8b97618e480c4eff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1532 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d96c75aa8b834bd58ae603cfa1bc0af3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_9708\\3691189897.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [192/192 00:16]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on Diverse Dataset ===\n",
      "loss        : 2.8910\n",
      "model_preparation_time: 0.0000\n",
      "accuracy    : 0.5157\n",
      "precision   : 0.5100\n",
      "recall      : 0.8016\n",
      "specificity : 0.2298\n",
      "fpr         : 0.7702\n",
      "f1          : 0.6234\n",
      "mcc         : 0.0382\n",
      "kappa       : 0.0313\n",
      "mse         : 0.4843\n",
      "mae         : 0.4843\n",
      "auc         : 0.5658\n",
      "runtime     : 16.1556\n",
      "samples_per_second: 94.8280\n",
      "steps_per_second: 11.8840\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:48:41.443832Z",
     "start_time": "2025-07-16T17:48:40.899252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "dev_test_df = pd.read_csv(\"devign_with_event_trace.csv\")\n",
    "\n",
    "# Drop the 'func' column\n",
    "dev_test_df = dev_test_df.drop(columns=['func'])\n",
    "\n",
    "# Convert the 'target' column to integer (True → 1, False → 0)\n",
    "dev_test_df['target'] = dev_test_df['target'].astype(int)\n",
    "\n",
    "# Check the result\n",
    "dev_test_df.info()"
   ],
   "id": "2367efc3dd684c12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14031 entries, 0 to 14030\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   func_cleaned  14031 non-null  object\n",
      " 1   project       14031 non-null  object\n",
      " 2   target        14031 non-null  int64 \n",
      " 3   ast_path      14031 non-null  object\n",
      " 4   event_trace   14031 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 548.2+ KB\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:51:14.484256Z",
     "start_time": "2025-07-16T17:48:43.268975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "\n",
    "saved_dir = './cdl_unixcoder_model_final'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dev_test_df = dev_test_df.rename(columns={\"func_cleaned\": \"func\", \"target\": \"label\"})\n",
    "dev_test_df = dev_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "dev_test_ds = Dataset.from_pandas(dev_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"event_trace\"]] ,\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "devv_test_tok = dev_test_ds.map(tokenize_fn, batched=True)\n",
    "devv_test_tok = devv_test_tok.rename_column(\"label\", \"labels\")\n",
    "devv_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=devv_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on Devign Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "54237b11cc672d51",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/14031 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0ce494427d4436287f1c04e91fcdb4f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_9708\\2016617922.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1754' max='1754' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1754/1754 02:28]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on Devign Dataset ===\n",
      "loss        : 0.8693\n",
      "model_preparation_time: 0.0020\n",
      "accuracy    : 0.4300\n",
      "precision   : 0.4271\n",
      "recall      : 0.9775\n",
      "specificity : 0.0214\n",
      "fpr         : 0.9786\n",
      "f1          : 0.5945\n",
      "mcc         : -0.0037\n",
      "kappa       : -0.0009\n",
      "mse         : 0.5700\n",
      "mae         : 0.5700\n",
      "auc         : 0.5134\n",
      "runtime     : 148.2997\n",
      "samples_per_second: 94.6120\n",
      "steps_per_second: 11.8270\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:52:31.533226Z",
     "start_time": "2025-07-16T17:52:31.412788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "juliet_test_df = pd.read_csv(\"djuliet_with_event_trace.csv\")\n",
    "juliet_test_df.info()"
   ],
   "id": "5954200f7ccef129",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3152 entries, 0 to 3151\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   code_snip    3152 non-null   object\n",
      " 1   output       3152 non-null   int64 \n",
      " 2   graph_path   3152 non-null   object\n",
      " 3   event_trace  3152 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 98.6+ KB\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:53:13.371482Z",
     "start_time": "2025-07-16T17:52:33.079963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './cdl_unixcoder_model_final'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "juliet_test_df = juliet_test_df.rename(columns={\"code_snip\": \"func\", \"output\": \"label\"})\n",
    "juliet_test_df = juliet_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "SEP = \" // LOGIC: \"\n",
    "juliet_test_df['model_input'] =juliet_test_df['func'] + SEP +  juliet_test_df['event_trace']\n",
    "# Convert to HuggingFace Dataset\n",
    "juliet_test_ds = Dataset.from_pandas(juliet_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"event_trace\"]] ,\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "juliet_test_tok = juliet_test_ds.map(tokenize_fn, batched=True)\n",
    "juliet_test_tok = juliet_test_tok.rename_column(\"label\", \"labels\")\n",
    "juliet_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=juliet_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on juliet Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "d47b7a4238b3f93",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3152 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73396dbb681245a6944aa41f0abc0135"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_9708\\2822260897.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='394' max='394' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [394/394 00:37]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on juliet Dataset ===\n",
      "loss        : 3.7804\n",
      "model_preparation_time: 0.0020\n",
      "accuracy    : 0.4946\n",
      "precision   : 0.4972\n",
      "recall      : 0.9429\n",
      "specificity : 0.0463\n",
      "fpr         : 0.9537\n",
      "f1          : 0.6510\n",
      "mcc         : -0.0244\n",
      "kappa       : -0.0108\n",
      "mse         : 0.5054\n",
      "mae         : 0.5054\n",
      "auc         : 0.5848\n",
      "runtime     : 37.8338\n",
      "samples_per_second: 83.3120\n",
      "steps_per_second: 10.4140\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:53:13.534359Z",
     "start_time": "2025-07-16T17:53:13.463314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "reveal_test_df = pd.read_csv(\"reveal_with_event_trace.csv\")\n",
    "reveal_test_df.info()"
   ],
   "id": "f2cdbaf5251122cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2028 entries, 0 to 2027\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  2028 non-null   object\n",
      " 1   input        2028 non-null   object\n",
      " 2   output       2028 non-null   int64 \n",
      " 3   graph_path   2028 non-null   object\n",
      " 4   event_trace  2028 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 79.3+ KB\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:53:39.890289Z",
     "start_time": "2025-07-16T17:53:13.585156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './cdl_unixcoder_model_final'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "reveal_test_df = reveal_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "reveal_test_df = reveal_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "SEP = \" // LOGIC: \"\n",
    "reveal_test_df['model_input'] =reveal_test_df['func'] + SEP +  reveal_test_df['event_trace']\n",
    "# Convert to HuggingFace Dataset\n",
    "reveal_test_ds = Dataset.from_pandas(reveal_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"event_trace\"]] ,\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "reveal_test_tok = reveal_test_ds.map(tokenize_fn, batched=True)\n",
    "reveal_test_tok = reveal_test_tok.rename_column(\"label\", \"labels\")\n",
    "reveal_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=reveal_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on RVl Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "91e31b7241116794",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2028 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7c15ec856674730989eb829bad3fe1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_9708\\919706373.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='254' max='254' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [254/254 00:24]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on RVl Dataset ===\n",
      "loss        : 3.3378\n",
      "model_preparation_time: 0.0098\n",
      "accuracy    : 0.4970\n",
      "precision   : 0.4983\n",
      "recall      : 0.8432\n",
      "specificity : 0.1509\n",
      "fpr         : 0.8491\n",
      "f1          : 0.6264\n",
      "mcc         : -0.0082\n",
      "kappa       : -0.0059\n",
      "mse         : 0.5030\n",
      "mae         : 0.5030\n",
      "auc         : 0.5191\n",
      "runtime     : 24.9384\n",
      "samples_per_second: 81.3200\n",
      "steps_per_second: 10.1850\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:54:36.121199Z",
     "start_time": "2025-07-16T17:54:35.989929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "cv_test_df = pd.read_csv(\"cvefixes_with_event_trace.csv\")\n",
    "cv_test_df.info()"
   ],
   "id": "624a7f0c2506d6d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4216 entries, 0 to 4215\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  4216 non-null   object\n",
      " 1   input        4216 non-null   object\n",
      " 2   output       4216 non-null   int64 \n",
      " 3   idx          4216 non-null   int64 \n",
      " 4   graph_path   4216 non-null   object\n",
      " 5   event_trace  4216 non-null   object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 197.8+ KB\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:55:29.655096Z",
     "start_time": "2025-07-16T17:54:37.694459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './cdl_unixcoder_model_final'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "cv_test_df = cv_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "cv_test_df = cv_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "SEP = \" // LOGIC: \"\n",
    "cv_test_df['model_input'] =cv_test_df['func'] + SEP +  cv_test_df['event_trace']\n",
    "# Convert to HuggingFace Dataset\n",
    "cv_test_ds = Dataset.from_pandas(cv_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"event_trace\"]] ,\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "cv_test_tok = cv_test_ds.map(tokenize_fn, batched=True)\n",
    "cv_test_tok = cv_test_tok.rename_column(\"label\", \"labels\")\n",
    "cv_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=cv_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on RVl Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "adc19dc2c2375ddb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4216 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd38b8b38d7943feaa6b3e06165cb52e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_9708\\1017571098.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='527' max='527' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [527/527 00:49]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on RVl Dataset ===\n",
      "loss        : 3.5527\n",
      "model_preparation_time: 0.0000\n",
      "accuracy    : 0.5064\n",
      "precision   : 0.5034\n",
      "recall      : 0.9398\n",
      "specificity : 0.0731\n",
      "fpr         : 0.9269\n",
      "f1          : 0.6556\n",
      "mcc         : 0.0257\n",
      "kappa       : 0.0128\n",
      "mse         : 0.4936\n",
      "mae         : 0.4936\n",
      "auc         : 0.5061\n",
      "runtime     : 49.7739\n",
      "samples_per_second: 84.7030\n",
      "steps_per_second: 10.5880\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:55:43.653845Z",
     "start_time": "2025-07-16T17:55:43.567502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "CV_test_df = pd.read_csv(\"mixvul_with_event_trace.csv\")\n",
    "CV_test_df.info()"
   ],
   "id": "c9ab09389dc8ac13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2864 entries, 0 to 2863\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  2864 non-null   object\n",
      " 1   input        2864 non-null   object\n",
      " 2   output       2864 non-null   int64 \n",
      " 3   graph_path   2864 non-null   object\n",
      " 4   event_trace  2864 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 112.0+ KB\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:56:21.144759Z",
     "start_time": "2025-07-16T17:55:44.939958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './cdl_unixcoder_model_final'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "CV_test_df = CV_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "CV_test_df = CV_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "SEP = \" // LOGIC: \"\n",
    "CV_test_df['model_input'] =CV_test_df['func'] + SEP +  CV_test_df['event_trace']\n",
    "# Convert to HuggingFace Dataset\n",
    "CV_test_ds = Dataset.from_pandas(CV_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"event_trace\"]] ,\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "CV_test_tok = CV_test_ds.map(tokenize_fn, batched=True)\n",
    "CV_test_tok = CV_test_tok.rename_column(\"label\", \"labels\")\n",
    "CV_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=CV_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on RVl Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "dcef537931520672",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2864 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "729702ac1f4d49809af5de91caa0231c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_9708\\325202402.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='358' max='358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [358/358 00:34]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on RVl Dataset ===\n",
      "loss        : 3.0686\n",
      "model_preparation_time: 0.0000\n",
      "accuracy    : 0.5129\n",
      "precision   : 0.5077\n",
      "recall      : 0.8471\n",
      "specificity : 0.1788\n",
      "fpr         : 0.8212\n",
      "f1          : 0.6349\n",
      "mcc         : 0.0347\n",
      "kappa       : 0.0258\n",
      "mse         : 0.4871\n",
      "mae         : 0.4871\n",
      "auc         : 0.5766\n",
      "runtime     : 34.5757\n",
      "samples_per_second: 82.8330\n",
      "steps_per_second: 10.3540\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "12f6503303330bc5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
