{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:13:58.789188Z",
     "start_time": "2025-07-16T17:13:54.076651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "gbig_test_df = pd.read_csv(\"big_vul_with_event_trace.csv\")\n",
    "\n",
    "gbig_test_df.info()"
   ],
   "id": "5defb080f2d4b2fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1170 entries, 0 to 1169\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  1170 non-null   object\n",
      " 1   input        1170 non-null   object\n",
      " 2   output       1170 non-null   int64 \n",
      " 3   graph_path   1170 non-null   object\n",
      " 4   event_trace  1170 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 45.8+ KB\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:18:42.252788Z",
     "start_time": "2025-07-16T17:18:28.952283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './cdl_codebert_model_final'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "gbig_test_df = gbig_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "gbig_test_df = gbig_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "SEP = \" // LOGIC: \"\n",
    "gbig_test_df['model_input'] =gbig_test_df['func'] + SEP +  gbig_test_df['event_trace']\n",
    "# Convert to HuggingFace Dataset\n",
    "gbig_test_ds = Dataset.from_pandas(gbig_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"event_trace\"]] ,\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "big_test_tok = gbig_test_ds.map(tokenize_fn, batched=True)\n",
    "big_test_tok = big_test_tok.rename_column(\"label\", \"labels\")\n",
    "big_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=big_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on BigVul Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "bc631408484cb948",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1170 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "03f791c649884df098a1dbd8386c350e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_21472\\1716570136.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [147/147 00:10]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on BigVul Dataset ===\n",
      "loss        : 0.7756\n",
      "model_preparation_time: 0.0040\n",
      "accuracy    : 0.5111\n",
      "precision   : 0.5305\n",
      "recall      : 0.1932\n",
      "specificity : 0.8291\n",
      "fpr         : 0.1709\n",
      "f1          : 0.2832\n",
      "mcc         : 0.0288\n",
      "kappa       : 0.0222\n",
      "mse         : 0.4889\n",
      "mae         : 0.4889\n",
      "auc         : 0.5343\n",
      "runtime     : 11.3541\n",
      "samples_per_second: 103.0470\n",
      "steps_per_second: 12.9470\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:19:52.014108Z",
     "start_time": "2025-07-16T17:19:51.952840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "gdiv_test_df = pd.read_csv(\"diverse_with_event_trace_graph.csv\")\n",
    "gdiv_test_df.info()"
   ],
   "id": "d6532ed846709702",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1532 entries, 0 to 1531\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   code_snip    1532 non-null   object\n",
      " 1   output       1532 non-null   int64 \n",
      " 2   graph_path   1532 non-null   object\n",
      " 3   event_trace  1532 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 48.0+ KB\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:20:10.766984Z",
     "start_time": "2025-07-16T17:19:54.032260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './cdl_codebert_model_final'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "gdiv_test_df = gdiv_test_df.rename(columns={\"code_snip\": \"func\", \"output\": \"label\"})\n",
    "gdiv_test_df = gdiv_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "SEP = \" // LOGIC: \"\n",
    "gdiv_test_df['model_input'] =gdiv_test_df['func'] + SEP +  gdiv_test_df['event_trace']\n",
    "# Convert to HuggingFace Dataset\n",
    "gdiv_test_ds = Dataset.from_pandas(gdiv_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"event_trace\"]] ,\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "div_test_tok = gdiv_test_ds.map(tokenize_fn, batched=True)\n",
    "div_test_tok = div_test_tok.rename_column(\"label\", \"labels\")\n",
    "div_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=div_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on Diverse Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "8b97618e480c4eff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1532 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c17cf8c184e14fa9b1dbf491965b48a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_21472\\541977314.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [192/192 00:15]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on Diverse Dataset ===\n",
      "loss        : 0.8824\n",
      "model_preparation_time: 0.0070\n",
      "accuracy    : 0.5313\n",
      "precision   : 0.5556\n",
      "recall      : 0.3133\n",
      "specificity : 0.7493\n",
      "fpr         : 0.2507\n",
      "f1          : 0.4007\n",
      "mcc         : 0.0696\n",
      "kappa       : 0.0627\n",
      "mse         : 0.4687\n",
      "mae         : 0.4687\n",
      "auc         : 0.5317\n",
      "runtime     : 15.4585\n",
      "samples_per_second: 99.1040\n",
      "steps_per_second: 12.4200\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:22:15.871460Z",
     "start_time": "2025-07-16T17:22:15.315933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "dev_test_df = pd.read_csv(\"devign_with_event_trace.csv\")\n",
    "\n",
    "# Drop the 'func' column\n",
    "dev_test_df = dev_test_df.drop(columns=['func'])\n",
    "\n",
    "# Convert the 'target' column to integer (True → 1, False → 0)\n",
    "dev_test_df['target'] = dev_test_df['target'].astype(int)\n",
    "\n",
    "# Check the result\n",
    "dev_test_df.info()"
   ],
   "id": "2367efc3dd684c12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14031 entries, 0 to 14030\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   func_cleaned  14031 non-null  object\n",
      " 1   project       14031 non-null  object\n",
      " 2   target        14031 non-null  int64 \n",
      " 3   ast_path      14031 non-null  object\n",
      " 4   event_trace   14031 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 548.2+ KB\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:25:03.740735Z",
     "start_time": "2025-07-16T17:22:46.815942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "\n",
    "saved_dir = './cdl_codebert_model_final'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dev_test_df = dev_test_df.rename(columns={\"func_cleaned\": \"func\", \"target\": \"label\"})\n",
    "dev_test_df = dev_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "dev_test_ds = Dataset.from_pandas(dev_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"event_trace\"]] ,\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "devv_test_tok = dev_test_ds.map(tokenize_fn, batched=True)\n",
    "devv_test_tok = devv_test_tok.rename_column(\"label\", \"labels\")\n",
    "devv_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=devv_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on Devign Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "54237b11cc672d51",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/14031 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9d9b59ac2cf4aac878ecc24363219b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_21472\\1693829858.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1754' max='1754' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1754/1754 02:13]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on Devign Dataset ===\n",
      "loss        : 0.6864\n",
      "model_preparation_time: 0.0000\n",
      "accuracy    : 0.5731\n",
      "precision   : 0.5134\n",
      "recall      : 0.0223\n",
      "specificity : 0.9842\n",
      "fpr         : 0.0158\n",
      "f1          : 0.0428\n",
      "mcc         : 0.0239\n",
      "kappa       : 0.0074\n",
      "mse         : 0.4269\n",
      "mae         : 0.4269\n",
      "auc         : 0.5127\n",
      "runtime     : 134.1288\n",
      "samples_per_second: 104.6080\n",
      "steps_per_second: 13.0770\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:25:31.713072Z",
     "start_time": "2025-07-16T17:25:31.633945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "juliet_test_df = pd.read_csv(\"djuliet_with_event_trace.csv\")\n",
    "juliet_test_df.info()"
   ],
   "id": "5954200f7ccef129",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3152 entries, 0 to 3151\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   code_snip    3152 non-null   object\n",
      " 1   output       3152 non-null   int64 \n",
      " 2   graph_path   3152 non-null   object\n",
      " 3   event_trace  3152 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 98.6+ KB\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:26:08.142574Z",
     "start_time": "2025-07-16T17:25:33.724540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './cdl_codebert_model_final'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "juliet_test_df = juliet_test_df.rename(columns={\"code_snip\": \"func\", \"output\": \"label\"})\n",
    "juliet_test_df = juliet_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "SEP = \" // LOGIC: \"\n",
    "juliet_test_df['model_input'] =juliet_test_df['func'] + SEP +  juliet_test_df['event_trace']\n",
    "# Convert to HuggingFace Dataset\n",
    "juliet_test_ds = Dataset.from_pandas(juliet_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"event_trace\"]] ,\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "juliet_test_tok = juliet_test_ds.map(tokenize_fn, batched=True)\n",
    "juliet_test_tok = juliet_test_tok.rename_column(\"label\", \"labels\")\n",
    "juliet_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=juliet_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on juliet Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "d47b7a4238b3f93",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3152 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8074222c2b648d9b9a6ce575c439bd3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_21472\\1658845022.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='394' max='394' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [394/394 00:32]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on juliet Dataset ===\n",
      "loss        : 0.9331\n",
      "model_preparation_time: 0.0154\n",
      "accuracy    : 0.5305\n",
      "precision   : 0.5625\n",
      "recall      : 0.2741\n",
      "specificity : 0.7868\n",
      "fpr         : 0.2132\n",
      "f1          : 0.3686\n",
      "mcc         : 0.0709\n",
      "kappa       : 0.0609\n",
      "mse         : 0.4695\n",
      "mae         : 0.4695\n",
      "auc         : 0.5227\n",
      "runtime     : 32.7777\n",
      "samples_per_second: 96.1630\n",
      "steps_per_second: 12.0200\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:27:20.909624Z",
     "start_time": "2025-07-16T17:27:20.818451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "reveal_test_df = pd.read_csv(\"reveal_with_event_trace.csv\")\n",
    "reveal_test_df.info()"
   ],
   "id": "f2cdbaf5251122cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2028 entries, 0 to 2027\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  2028 non-null   object\n",
      " 1   input        2028 non-null   object\n",
      " 2   output       2028 non-null   int64 \n",
      " 3   graph_path   2028 non-null   object\n",
      " 4   event_trace  2028 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 79.3+ KB\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:27:44.138924Z",
     "start_time": "2025-07-16T17:27:22.902024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './cdl_codebert_model_final'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "reveal_test_df = reveal_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "reveal_test_df = reveal_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "SEP = \" // LOGIC: \"\n",
    "reveal_test_df['model_input'] =reveal_test_df['func'] + SEP +  reveal_test_df['event_trace']\n",
    "# Convert to HuggingFace Dataset\n",
    "reveal_test_ds = Dataset.from_pandas(reveal_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"event_trace\"]] ,\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "reveal_test_tok = reveal_test_ds.map(tokenize_fn, batched=True)\n",
    "reveal_test_tok = reveal_test_tok.rename_column(\"label\", \"labels\")\n",
    "reveal_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=reveal_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on RVl Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "91e31b7241116794",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2028 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "928345e4f02748a9aba043404ae45d5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_21472\\2120952220.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='254' max='254' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [254/254 00:19]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on RVl Dataset ===\n",
      "loss        : 0.9377\n",
      "model_preparation_time: 0.0081\n",
      "accuracy    : 0.4921\n",
      "precision   : 0.4839\n",
      "recall      : 0.2367\n",
      "specificity : 0.7475\n",
      "fpr         : 0.2525\n",
      "f1          : 0.3179\n",
      "mcc         : -0.0184\n",
      "kappa       : -0.0158\n",
      "mse         : 0.5079\n",
      "mae         : 0.5079\n",
      "auc         : 0.5107\n",
      "runtime     : 19.7643\n",
      "samples_per_second: 102.6090\n",
      "steps_per_second: 12.8510\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:28:56.399095Z",
     "start_time": "2025-07-16T17:28:56.280043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "cv_test_df = pd.read_csv(\"cvefixes_with_event_trace.csv\")\n",
    "cv_test_df.info()"
   ],
   "id": "624a7f0c2506d6d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4216 entries, 0 to 4215\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  4216 non-null   object\n",
      " 1   input        4216 non-null   object\n",
      " 2   output       4216 non-null   int64 \n",
      " 3   idx          4216 non-null   int64 \n",
      " 4   graph_path   4216 non-null   object\n",
      " 5   event_trace  4216 non-null   object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 197.8+ KB\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:29:42.284659Z",
     "start_time": "2025-07-16T17:28:59.074008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './cdl_codebert_model_final'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "cv_test_df = cv_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "cv_test_df = cv_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "SEP = \" // LOGIC: \"\n",
    "cv_test_df['model_input'] =cv_test_df['func'] + SEP +  cv_test_df['event_trace']\n",
    "# Convert to HuggingFace Dataset\n",
    "cv_test_ds = Dataset.from_pandas(cv_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"event_trace\"]] ,\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "cv_test_tok = cv_test_ds.map(tokenize_fn, batched=True)\n",
    "cv_test_tok = cv_test_tok.rename_column(\"label\", \"labels\")\n",
    "cv_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=cv_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on RVl Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "adc19dc2c2375ddb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/4216 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e361369f8e0d492895434a225cb3c77d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_21472\\2160724805.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='527' max='527' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [527/527 00:41]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on RVl Dataset ===\n",
      "loss        : 0.9430\n",
      "model_preparation_time: 0.0060\n",
      "accuracy    : 0.5047\n",
      "precision   : 0.5062\n",
      "recall      : 0.3857\n",
      "specificity : 0.6238\n",
      "fpr         : 0.3762\n",
      "f1          : 0.4378\n",
      "mcc         : 0.0098\n",
      "kappa       : 0.0095\n",
      "mse         : 0.4953\n",
      "mae         : 0.4953\n",
      "auc         : 0.5039\n",
      "runtime     : 41.1136\n",
      "samples_per_second: 102.5450\n",
      "steps_per_second: 12.8180\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:30:28.150804Z",
     "start_time": "2025-07-16T17:30:28.064069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "CV_test_df = pd.read_csv(\"mixvul_with_event_trace.csv\")\n",
    "CV_test_df.info()"
   ],
   "id": "c9ab09389dc8ac13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2864 entries, 0 to 2863\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  2864 non-null   object\n",
      " 1   input        2864 non-null   object\n",
      " 2   output       2864 non-null   int64 \n",
      " 3   graph_path   2864 non-null   object\n",
      " 4   event_trace  2864 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 112.0+ KB\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T17:31:00.612530Z",
     "start_time": "2025-07-16T17:30:30.171857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './cdl_codebert_model_final'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "CV_test_df = CV_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "CV_test_df = CV_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "SEP = \" // LOGIC: \"\n",
    "CV_test_df['model_input'] =CV_test_df['func'] + SEP +  CV_test_df['event_trace']\n",
    "# Convert to HuggingFace Dataset\n",
    "CV_test_ds = Dataset.from_pandas(CV_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"event_trace\"]] ,\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "CV_test_tok = CV_test_ds.map(tokenize_fn, batched=True)\n",
    "CV_test_tok = CV_test_tok.rename_column(\"label\", \"labels\")\n",
    "CV_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=CV_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on RVl Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "dcef537931520672",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2864 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66ba670470744c4abd1800ee47ca639d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_21472\\314238359.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='358' max='358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [358/358 00:28]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on RVl Dataset ===\n",
      "loss        : 0.8386\n",
      "model_preparation_time: 0.0020\n",
      "accuracy    : 0.5468\n",
      "precision   : 0.5821\n",
      "recall      : 0.3317\n",
      "specificity : 0.7619\n",
      "fpr         : 0.2381\n",
      "f1          : 0.4226\n",
      "mcc         : 0.1037\n",
      "kappa       : 0.0936\n",
      "mse         : 0.4532\n",
      "mae         : 0.4532\n",
      "auc         : 0.5799\n",
      "runtime     : 28.6846\n",
      "samples_per_second: 99.8440\n",
      "steps_per_second: 12.4810\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "12f6503303330bc5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
