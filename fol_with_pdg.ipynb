{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T21:30:29.419115Z",
     "start_time": "2025-05-23T21:30:29.378923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "import os, shutil\n",
    "import pandas as pd\n",
    "from clang.cindex import Config, Index, CursorKind\n",
    "\n",
    "\n",
    "# Paths\n",
    "CSV_IN      = \"devign.csv\"\n",
    "CSV_OUT     = \"devign_with_atoms.csv\"\n",
    "SNIPPET_DIR = \"snippets_clang\"\n",
    "\n",
    "\n",
    "assert os.path.isfile(CSV_IN), f\"Cannot find {CSV_IN}\"\n",
    "index = Index.create()\n",
    "print(\"Setup OK\")\n"
   ],
   "id": "a59cd5fd88126fec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup OK\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T21:31:08.392360Z",
     "start_time": "2025-05-23T21:30:51.206229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "df = pd.read_csv(CSV_IN)\n",
    "df['id'] = df.index\n",
    "\n",
    "\n",
    "shutil.rmtree(SNIPPET_DIR, ignore_errors=True)\n",
    "os.makedirs(SNIPPET_DIR, exist_ok=True)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    path = os.path.join(SNIPPET_DIR, f\"snippet_{i}.c\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(row[\"func_cleaned\"])\n",
    "print(f\"Wrote {len(df)} snippets → {SNIPPET_DIR}\")\n"
   ],
   "id": "5579089040b34085",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 14032 snippets → snippets_clang\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T21:34:58.387169Z",
     "start_time": "2025-05-23T21:31:13.001823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def extract_atoms(path):\n",
    "    tu = index.parse(path, args=['-std=c99'])\n",
    "    calls, decls, params, casts, lits, guards = [], [], [], [], [], []\n",
    "\n",
    "    def walk(node):\n",
    "        # Calls\n",
    "        if node.kind == CursorKind.CALL_EXPR and node.spelling:\n",
    "            calls.append((node.spelling, node.location.line))\n",
    "        # Local decls\n",
    "        if node.kind == CursorKind.VAR_DECL and node.spelling:\n",
    "            decls.append((node.spelling, node.location.line))\n",
    "        # Params\n",
    "        if node.kind == CursorKind.PARM_DECL and node.spelling:\n",
    "            params.append((node.spelling, node.location.line))\n",
    "        # C-style casts\n",
    "        if node.kind == CursorKind.CSTYLE_CAST_EXPR:\n",
    "            casts.append((node.type.spelling, node.location.line))\n",
    "        # Literals\n",
    "        if node.kind in (\n",
    "            CursorKind.INTEGER_LITERAL,\n",
    "            CursorKind.FLOATING_LITERAL,\n",
    "            CursorKind.STRING_LITERAL\n",
    "        ):\n",
    "            code = \"\".join(tok.spelling for tok in node.get_tokens())\n",
    "            lits.append((code, node.location.line))\n",
    "        # If-guards\n",
    "        if node.kind == CursorKind.IF_STMT:\n",
    "            children = list(node.get_children())\n",
    "            if children:\n",
    "                cond = \"\".join(tok.spelling for tok in children[0].get_tokens())\n",
    "                guards.append((cond.replace(\" \", \"\"), node.location.line))\n",
    "        for c in node.get_children():\n",
    "            walk(c)\n",
    "    walk(tu.cursor)\n",
    "\n",
    "    atoms = []\n",
    "    atoms += [f\"Call(S,'{n}',{l})\"      for n,l in calls]\n",
    "    atoms += [f\"Decl(S,'{n}',{l})\"      for n,l in decls]\n",
    "    atoms += [f\"Param(S,'{n}',{l})\"     for n,l in params]\n",
    "    atoms += [f\"Cast(S,'{t}',{l})\"      for t,l in casts]\n",
    "    atoms += [f\"Literal(S,'{c}',{l})\"   for c,l in lits]\n",
    "    atoms += [f\"Guard(S,'if-{c}',{l},{l+1})\" for c,l in guards]\n",
    "    return atoms\n",
    "\n",
    "# Run across all snippets\n",
    "rows = []\n",
    "for i in df['id']:\n",
    "    file = os.path.join(SNIPPET_DIR, f\"snippet_{i}.c\")\n",
    "    for atom in extract_atoms(file):\n",
    "        rows.append((i, atom))\n",
    "print(\"Extracted\", len(rows), \"atoms\")\n"
   ],
   "id": "b8151b8090d82cff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 195889 atoms\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T21:38:37.941686Z",
     "start_time": "2025-05-23T21:38:35.680154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% Cell 4: Build fol_logic & save\n",
    "atoms_df = pd.DataFrame(rows, columns=['id','atom'])\n",
    "df['fol_logic'] = (\n",
    "    atoms_df.groupby('id')['atom']\n",
    "            .apply(lambda atoms: \", \".join(atoms))\n",
    "            .reindex(df['id'])\n",
    "            .fillna(\"\")\n",
    ")\n",
    "df.to_csv(CSV_OUT, index=False)\n",
    "print(\"Saved augmented CSV to\", CSV_OUT)\n"
   ],
   "id": "a98f83848f67f0db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved augmented CSV to devign_with_atoms.csv\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T00:10:50.979726Z",
     "start_time": "2025-05-24T00:10:50.052917Z"
    }
   },
   "cell_type": "code",
   "source": "data= pd.read_csv('devign_with_atoms.csv')",
   "id": "6f58bc7593c08e61",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T00:11:01.400457Z",
     "start_time": "2025-05-24T00:11:01.352916Z"
    }
   },
   "cell_type": "code",
   "source": "data.info()",
   "id": "4542a491f72e1976",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14032 entries, 0 to 14031\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   func          14032 non-null  object\n",
      " 1   func_cleaned  14032 non-null  object\n",
      " 2   project       14032 non-null  object\n",
      " 3   target        14032 non-null  bool  \n",
      " 4   id            14032 non-null  int64 \n",
      " 5   fol_logic     13736 non-null  object\n",
      "dtypes: bool(1), int64(1), object(4)\n",
      "memory usage: 562.0+ KB\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e49f3f395a2efae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "++++++++++++++",
   "id": "7d44f950d695192a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T01:49:53.612443Z",
     "start_time": "2025-05-24T01:49:52.787969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from clang.cindex import Config, Index, CursorKind\n",
    "\n",
    "# Set path to your libclang (update this path if necessary)\n",
    "Config.set_library_file(r\"C:\\Program Files\\LLVM\\bin\\libclang.dll\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────\n",
    "# CONFIGURATION\n",
    "# ─────────────────────────────────────────────────────\n",
    "CSV_IN = \"devign.csv\"\n",
    "CSV_OUT = \"devign_with_pdg_fol.csv\"\n",
    "SNIPPET_DIR = \"snippets_clang_pdg\"\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(CSV_IN)\n",
    "df['id'] = df.index\n",
    "\n",
    "# Create snippet files\n",
    "os.makedirs(SNIPPET_DIR, exist_ok=True)\n",
    "for i, row in df.iterrows():\n",
    "    path = os.path.join(SNIPPET_DIR, f\"snippet_{i}.c\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(row[\"func_cleaned\"])\n",
    "\n",
    "# Initialize Clang index\n",
    "index = Index.create()\n",
    "\n",
    "# Function to extract PDG-style atoms\n",
    "def extract_pdg(path):\n",
    "    tu = index.parse(path, args=['-std=c99'])\n",
    "    data_deps = []\n",
    "    ctrl_deps = []\n",
    "    assigns = []\n",
    "    current_if = None\n",
    "\n",
    "    def walk(node, parent_if=None):\n",
    "        nonlocal current_if\n",
    "\n",
    "        # Assignment expression\n",
    "        if node.kind == CursorKind.BINARY_OPERATOR:\n",
    "            tokens = list(node.get_tokens())\n",
    "            expr = \" \".join(t.spelling for t in tokens)\n",
    "            m = re.match(r\"(\\w+)\\s*=\\s*(\\w+)\", expr)\n",
    "            if m:\n",
    "                lhs, rhs = m.group(1), m.group(2)\n",
    "                data_deps.append((rhs, lhs, node.location.line))\n",
    "                assigns.append((lhs, rhs, node.location.line))\n",
    "\n",
    "        # If-statement for control dependency\n",
    "        if node.kind == CursorKind.IF_STMT:\n",
    "            cond_node = list(node.get_children())[0]\n",
    "            cond_tokens = list(cond_node.get_tokens())\n",
    "            cond = \"\".join(tok.spelling for tok in cond_tokens)\n",
    "            current_if = (cond, node.location.line)\n",
    "\n",
    "        # Control dependencies for child tokens\n",
    "        if parent_if:\n",
    "            for tok in node.get_tokens():\n",
    "                ctrl_deps.append((parent_if[0], parent_if[1], node.location.line))\n",
    "\n",
    "        for child in node.get_children():\n",
    "            walk(child, current_if if node.kind == CursorKind.IF_STMT else parent_if)\n",
    "\n",
    "    walk(tu.cursor)\n",
    "\n",
    "    atoms = []\n",
    "    atoms += [f\"DataDep(S,'{src}','{dst}',{line})\" for src, dst, line in data_deps]\n",
    "    atoms += [f\"Assign(S,'{lhs}','{rhs}',{line})\" for lhs, rhs, line in assigns]\n",
    "    atoms += [f\"CtrlDep(S,'if-{cond}',{src},{dst})\" for cond, src, dst in ctrl_deps]\n",
    "    return atoms\n",
    "\n",
    "# Extract atoms for each snippet\n",
    "rows = []\n",
    "for i in df['id']:\n",
    "    file = os.path.join(SNIPPET_DIR, f\"snippet_{i}.c\")\n",
    "    for atom in extract_pdg(file):\n",
    "        rows.append((i, atom))\n",
    "\n",
    "# Merge atoms into DataFrame\n",
    "atoms_df = pd.DataFrame(rows, columns=['id', 'atom'])\n",
    "df['fol_logic'] = (\n",
    "    atoms_df.groupby('id')['atom']\n",
    "            .apply(lambda atoms: \", \".join(atoms))\n",
    "            .reindex(df['id'])\n",
    "            .fillna(\"\")\n",
    ")\n",
    "\n",
    "# Save updated CSV\n",
    "df.to_csv(CSV_OUT, index=False)\n",
    "\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"PDG-Enhanced Dataset\", dataframe=df)\n"
   ],
   "id": "5dae1eaf74f20fe",
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "library file must be set before before using any other functionalities in libclang.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mclang\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcindex\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Config, Index, CursorKind\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Set path to your libclang (update this path if necessary)\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m \u001B[43mConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_library_file\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mC:\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mProgram Files\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mLLVM\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mbin\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mlibclang.dll\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# ─────────────────────────────────────────────────────\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# CONFIGURATION\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# ─────────────────────────────────────────────────────\u001B[39;00m\n\u001B[0;32m     12\u001B[0m CSV_IN \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdevign.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pdg_data\\.venv\\lib\\site-packages\\clang\\cindex.py:4117\u001B[0m, in \u001B[0;36mConfig.set_library_file\u001B[1;34m(filename)\u001B[0m\n\u001B[0;32m   4115\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Set the exact location of libclang\"\"\"\u001B[39;00m\n\u001B[0;32m   4116\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m Config\u001B[38;5;241m.\u001B[39mloaded:\n\u001B[1;32m-> 4117\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n\u001B[0;32m   4118\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibrary file must be set before before using \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   4119\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124many other functionalities in libclang.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   4120\u001B[0m     )\n\u001B[0;32m   4122\u001B[0m Config\u001B[38;5;241m.\u001B[39mlibrary_file \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mfspath(filename)\n",
      "\u001B[1;31mException\u001B[0m: library file must be set before before using any other functionalities in libclang."
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data.columns",
   "id": "7933172fd9f3b7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "CODEPTM",
   "id": "e1b48261b68e246c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "CODEBERT",
   "id": "1bdb94d785ef137d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "FOL_PDF",
   "id": "1d17ef5d7bd05604"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T01:43:28.086849Z",
     "start_time": "2025-05-24T01:42:13.959091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────\n",
    "# 1) LOAD & PREPARE DATA\n",
    "# ───────────────────────────────────────────────────────────\n",
    "df = pd.read_csv(\"devign_with_atoms.csv\")        # must have 'fol_logic','func_cleaned','target'\n",
    "df['target'] = df['target'].astype(int)           # ensure labels are ints\n",
    "SEP = \" // LOGIC: \"\n",
    "df['model_input'] = df['fol_logic'].fillna('') + SEP + df['func_cleaned']\n",
    "\n",
    "# build HF Dataset and split 80/20\n",
    "hf_ds = Dataset.from_pandas(\n",
    "    df[['model_input','target']].rename(columns={'model_input':'text','target':'label'})\n",
    ")\n",
    "splits = hf_ds.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────\n",
    "# 2) TOKENIZATION\n",
    "# ───────────────────────────────────────────────────────────\n",
    "tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=256\n",
    "    )\n",
    "tokenized = splits.map(tokenize_fn, batched=True)\n",
    "tokenized = tokenized.rename_column('label','labels')\n",
    "tokenized.set_format('torch', columns=['input_ids','attention_mask','labels'])\n",
    "\n",
    "# ───────────────────────────────────────────────────────────\n",
    "# 3) LOAD MODEL & CONFIGURE LOSS\n",
    "# ───────────────────────────────────────────────────────────\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'microsoft/codebert-base',\n",
    "    num_labels=2\n",
    ")\n",
    "model.config.problem_type = \"single_label_classification\"\n",
    "model.config.id2label      = {0:\"safe\",1:\"vuln\"}\n",
    "model.config.label2id      = {\"safe\":0,\"vuln\":1}\n",
    "\n",
    "# ───────────────────────────────────────────────────────────\n",
    "# 4) METRICS\n",
    "# ───────────────────────────────────────────────────────────\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    return {\n",
    "        'accuracy':    accuracy_score(labels, preds),\n",
    "        'precision':   precision_score(labels, preds),\n",
    "        'recall':      recall_score(labels, preds),            # sensitivity\n",
    "        'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        'f1':          f1_score(labels, preds)\n",
    "    }\n",
    "\n",
    "# ───────────────────────────────────────────────────────────\n",
    "# 5) SET UP TRAINER\n",
    "# ───────────────────────────────────────────────────────────\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "args = TrainingArguments(\n",
    "    output_dir='./codebert_fol',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized['train'],\n",
    "    eval_dataset=tokenized['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────\n",
    "# 6) TRAIN & EVALUATE\n",
    "# ───────────────────────────────────────────────────────────\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "print(\"\\n=== Hold‐out Metrics ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────\n",
    "# 7) SAVE MODEL & TOKENIZER\n",
    "# ───────────────────────────────────────────────────────────\n",
    "save_dir = './codebert_fol_saved'\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(f\"\\nModel and tokenizer saved to {save_dir}\")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────\n",
    "# 8) LOAD & TEST ON NEW DATA\n",
    "# ───────────────────────────────────────────────────────────\n",
    "'''\n",
    "new_df = pd.read_csv('new_data.csv')\n",
    "new_df['model_input'] = new_df['fol_logic'].fillna('') + SEP + new_df['func_cleaned']\n",
    "new_ds = Dataset.from_pandas(new_df[['model_input']].rename(columns={'model_input':'text'}))\n",
    "\n",
    "# Reload saved artifacts\n",
    "saved_tokenizer = RobertaTokenizer.from_pretrained(save_dir)\n",
    "saved_model     = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "saved_model.eval()\n",
    "\n",
    "# Tokenize new data\n",
    "def tokenize_new(batch):\n",
    "    return saved_tokenizer(\n",
    "        batch['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=256\n",
    "    )\n",
    "new_tok = new_ds.map(tokenize_new, batched=True)\n",
    "new_tok.set_format('torch', columns=['input_ids','attention_mask'])\n",
    "\n",
    "# Inference DataLoader\n",
    "new_loader = torch.utils.data.DataLoader(\n",
    "    new_tok, batch_size=16, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# Predict\n",
    "preds = []\n",
    "for batch in new_loader:\n",
    "    with torch.no_grad():\n",
    "        out = saved_model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask']\n",
    "        )\n",
    "    preds.extend(torch.argmax(out.logits, axis=-1).cpu().tolist())\n",
    "\n",
    "new_df['prediction'] = preds\n",
    "print(\"\\nSample predictions on new data:\")\n",
    "print(new_df[['func_cleaned','prediction']].head())\n",
    "'''"
   ],
   "id": "a270e0b92b09bb10",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 11225/11225 [00:45<00:00, 244.75 examples/s]\n",
      "Map: 100%|██████████| 2807/2807 [00:21<00:00, 132.74 examples/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 50\u001B[0m\n\u001B[0;32m     45\u001B[0m tokenized\u001B[38;5;241m.\u001B[39mset_format(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m'\u001B[39m, columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     47\u001B[0m \u001B[38;5;66;03m# ───────────────────────────────────────────────────────────\u001B[39;00m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;66;03m# 3) LOAD MODEL & CONFIGURE LOSS\u001B[39;00m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;66;03m# ───────────────────────────────────────────────────────────\u001B[39;00m\n\u001B[1;32m---> 50\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForSequenceClassification\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmicrosoft/codebert-base\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_labels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\n\u001B[0;32m     53\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     54\u001B[0m model\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mproblem_type \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msingle_label_classification\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     55\u001B[0m model\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mid2label      \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m0\u001B[39m:\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msafe\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;241m1\u001B[39m:\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvuln\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n",
      "File \u001B[1;32m~\\PycharmProjects\\pdg_data\\.venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mconfig_class \u001B[38;5;241m==\u001B[39m config\u001B[38;5;241m.\u001B[39msub_configs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext_config\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    570\u001B[0m         config \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mget_text_config()\n\u001B[1;32m--> 571\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m    572\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    573\u001B[0m     )\n\u001B[0;32m    574\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    575\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    576\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    577\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\pdg_data\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:309\u001B[0m, in \u001B[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    307\u001B[0m old_dtype \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mget_default_dtype()\n\u001B[0;32m    308\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 309\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    310\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    311\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_default_dtype(old_dtype)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pdg_data\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:4573\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   4563\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_orig \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   4564\u001B[0m         torch\u001B[38;5;241m.\u001B[39mset_default_dtype(dtype_orig)\n\u001B[0;32m   4566\u001B[0m     (\n\u001B[0;32m   4567\u001B[0m         model,\n\u001B[0;32m   4568\u001B[0m         missing_keys,\n\u001B[0;32m   4569\u001B[0m         unexpected_keys,\n\u001B[0;32m   4570\u001B[0m         mismatched_keys,\n\u001B[0;32m   4571\u001B[0m         offload_index,\n\u001B[0;32m   4572\u001B[0m         error_msgs,\n\u001B[1;32m-> 4573\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_pretrained_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   4574\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4575\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4576\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheckpoint_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4577\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4578\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4579\u001B[0m \u001B[43m        \u001B[49m\u001B[43msharded_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msharded_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4580\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4581\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisk_offload_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4582\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffload_state_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_state_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4583\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4584\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4585\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4586\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4587\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4588\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweights_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4589\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4591\u001B[0m \u001B[38;5;66;03m# record tp degree the model sharded to\u001B[39;00m\n\u001B[0;32m   4592\u001B[0m model\u001B[38;5;241m.\u001B[39m_tp_size \u001B[38;5;241m=\u001B[39m tp_size\n",
      "File \u001B[1;32m~\\PycharmProjects\\pdg_data\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:4832\u001B[0m, in \u001B[0;36mPreTrainedModel._load_pretrained_model\u001B[1;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001B[0m\n\u001B[0;32m   4829\u001B[0m     original_checkpoint_keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(state_dict\u001B[38;5;241m.\u001B[39mkeys())\n\u001B[0;32m   4830\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   4831\u001B[0m     original_checkpoint_keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\n\u001B[1;32m-> 4832\u001B[0m         \u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint_files\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmeta\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweights_only\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mkeys()\n\u001B[0;32m   4833\u001B[0m     )\n\u001B[0;32m   4835\u001B[0m \u001B[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001B[39;00m\n\u001B[0;32m   4836\u001B[0m prefix \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mbase_model_prefix\n",
      "File \u001B[1;32m~\\PycharmProjects\\pdg_data\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:553\u001B[0m, in \u001B[0;36mload_state_dict\u001B[1;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001B[0m\n\u001B[0;32m    551\u001B[0m \u001B[38;5;66;03m# Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\u001B[39;00m\n\u001B[0;32m    552\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m weights_only:\n\u001B[1;32m--> 553\u001B[0m     \u001B[43mcheck_torch_load_is_safe\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    554\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    555\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m map_location \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\pdg_data\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1417\u001B[0m, in \u001B[0;36mcheck_torch_load_is_safe\u001B[1;34m()\u001B[0m\n\u001B[0;32m   1415\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcheck_torch_load_is_safe\u001B[39m():\n\u001B[0;32m   1416\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_greater_or_equal(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2.6\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m-> 1417\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1418\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1419\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1420\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwhen loading files with safetensors.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1421\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1422\u001B[0m         )\n",
      "\u001B[1;31mValueError\u001B[0m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
