{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T08:16:27.400095Z",
     "start_time": "2025-06-30T08:16:26.813714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_csv('/Users/akter/ns_main/dataset/nasa_test.csv')\n",
    "train_df = pd.read_csv('/Users/akter/ns_main/dataset/nasa_train.csv')"
   ],
   "id": "cdc3440a21767b49",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T08:16:27.717951Z",
     "start_time": "2025-06-30T08:16:27.565392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv('nasa_train.csv')\n",
    "test_df = pd.read_csv('nasa_test.csv')\n",
    "\n",
    "# Basic info: rows, columns, data types, nulls\n",
    "print(\"=== Train Dataset Info ===\")\n",
    "print(train_df.info())\n",
    "print(\"\\n=== Test Dataset Info ===\")\n",
    "print(test_df.info())\n",
    "\n",
    "# Column names\n",
    "print(\"\\nTrain Columns:\", train_df.columns.tolist())\n",
    "print(\"Test Columns:\", test_df.columns.tolist())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in Train:\\n\", train_df.isnull().sum())\n",
    "print(\"Missing values in Test:\\n\", test_df.isnull().sum())\n",
    "\n",
    "# First few rows\n",
    "print(\"\\nTrain Sample Rows:\\n\", train_df.head())\n",
    "print(\"\\nTest Sample Rows:\\n\", test_df.head())\n",
    "\n",
    "# Class distribution if 'label' column exists\n",
    "if 'label' in train_df.columns:\n",
    "    print(\"\\nTrain Label Distribution:\\n\", train_df['label'].value_counts())\n",
    "if 'label' in test_df.columns:\n",
    "    print(\"\\nTest Label Distribution:\\n\", test_df['label'].value_counts())\n",
    "\n",
    "# Dataset shape\n",
    "print(f\"\\nTrain Shape: {train_df.shape}\")\n",
    "print(f\"Test Shape: {test_df.shape}\")\n"
   ],
   "id": "752f7d0480187a63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Train Dataset Info ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9744 entries, 0 to 9743\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   repo    4919 non-null   object\n",
      " 1   func    9744 non-null   object\n",
      " 2   label   9744 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 228.5+ KB\n",
      "None\n",
      "\n",
      "=== Test Dataset Info ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2437 entries, 0 to 2436\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   repo    1262 non-null   object\n",
      " 1   func    2437 non-null   object\n",
      " 2   label   2437 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 57.2+ KB\n",
      "None\n",
      "\n",
      "Train Columns: ['repo', 'func', 'label']\n",
      "Test Columns: ['repo', 'func', 'label']\n",
      "\n",
      "Missing values in Train:\n",
      " repo     4825\n",
      "func        0\n",
      "label       0\n",
      "dtype: int64\n",
      "Missing values in Test:\n",
      " repo     1175\n",
      "func        0\n",
      "label       0\n",
      "dtype: int64\n",
      "\n",
      "Train Sample Rows:\n",
      "       repo                                               func  label\n",
      "0      NaN  void ConfigureModule(int pkt, int buffer) {\\n ...      0\n",
      "1  icarous  static void mavlink_test_nav_controller_output...      1\n",
      "2      NaN  void UpdateState(int input, int flag) {\\n    i...      0\n",
      "3      NaN  void CheckStatus(int input, int cmd) {\\n    in...      0\n",
      "4       CF  int CF_Traverse_WriteHistoryQueueEntryToFile(C...      1\n",
      "\n",
      "Test Sample Rows:\n",
      "    repo                                               func  label\n",
      "0  QuIP  static COMMAND_FUNC( do_list_serial_ports ){li...      1\n",
      "1  osal  void UtTest_Setup(void){    ADD_TEST(OS_Module...      1\n",
      "2   NaN  void ValidateInput(int pkt, int flag) {\\n    i...      0\n",
      "3   NaN  void UpdateState(int event, int buffer) {\\n   ...      0\n",
      "4    CF  void Test_CF_DoPurgeQueue_HistoryOnly(void)\\n{...      1\n",
      "\n",
      "Train Label Distribution:\n",
      " label\n",
      "1    4919\n",
      "0    4825\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test Label Distribution:\n",
      " label\n",
      "1    1262\n",
      "0    1175\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train Shape: (9744, 3)\n",
      "Test Shape: (2437, 3)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T08:16:29.514505Z",
     "start_time": "2025-06-30T08:16:29.509335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Drop 'repo' column from both train and test datasets\n",
    "train_df = train_df.drop(columns=['repo'])\n",
    "test_df = test_df.drop(columns=['repo'])"
   ],
   "id": "6bce085555f16164",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T08:16:30.953624Z",
     "start_time": "2025-06-30T08:16:30.948516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(train_df.columns)\n",
    "print(test_df.columns)\n"
   ],
   "id": "6c95afb6b7f86528",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['func', 'label'], dtype='object')\n",
      "Index(['func', 'label'], dtype='object')\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T08:51:44.682526Z",
     "start_time": "2025-06-30T08:17:30.208793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, matthews_corrcoef, cohen_kappa_score,\n",
    "    mean_squared_error, mean_absolute_error\n",
    ")\n",
    "\n",
    "# === Load and prepare the data ===\n",
    "train_df = pd.read_csv(\"nasa_train.csv\")[[\"func\", \"label\"]].dropna()\n",
    "test_df = pd.read_csv(\"nasa_test.csv\")[[\"func\", \"label\"]].dropna()\n",
    "train_ds = Dataset.from_pandas(train_df.rename(columns={\"func\": \"text\"}))\n",
    "test_ds = Dataset.from_pandas(test_df.rename(columns={\"func\": \"text\"}))\n",
    "\n",
    "# === Load tokenizer and model ===\n",
    "model_name = \"microsoft/unixcoder-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# === Tokenization function ===\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "test_ds = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "# === Compute metrics ===\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         roc_auc_score(labels, preds)\n",
    "    }\n",
    "\n",
    "# === Training arguments ===\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./graphcodebert_nasa_output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n",
    "\n",
    "# === Trainer ===\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# === Train the model ===\n",
    "trainer.train()\n",
    "\n",
    "# === Save the fine-tuned model and tokenizer ===\n",
    "save_dir = \"./unixcoder_nasa_finetuned\"\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "# === Evaluate and print results ===\n",
    "metrics = trainer.evaluate()\n",
    "print(\"\\n=== Evaluation Metrics ===\")\n",
    "for k, v in metrics.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    else:\n",
    "        print(f\"{k}: {v}\")\n"
   ],
   "id": "f0bb1bccd7cc0b29",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaon/.virtualenvs/PythonProject2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/unixcoder-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 7000/9744 [00:01<00:00, 6444.74 examples/s]Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9744/9744 [00:01<00:00, 5839.54 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2437/2437 [00:00<00:00, 6595.78 examples/s]\n",
      "/home/shaon/.virtualenvs/PythonProject2/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_669099/2724056354.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3045' max='3045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3045/3045 33:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Fpr</th>\n",
       "      <th>F1</th>\n",
       "      <th>Mcc</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='153' max='153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [153/153 00:29]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Metrics ===\n",
      "eval_loss: 0.0000\n",
      "eval_accuracy: 1.0000\n",
      "eval_precision: 1.0000\n",
      "eval_recall: 1.0000\n",
      "eval_specificity: 1.0000\n",
      "eval_fpr: 0.0000\n",
      "eval_f1: 1.0000\n",
      "eval_mcc: 1.0000\n",
      "eval_kappa: 1.0000\n",
      "eval_mse: 0.0000\n",
      "eval_mae: 0.0000\n",
      "eval_auc: 1.0000\n",
      "eval_runtime: 30.3669\n",
      "eval_samples_per_second: 80.2520\n",
      "eval_steps_per_second: 5.0380\n",
      "epoch: 5.0000\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "CV_test_df= pd.read_csv('/Users/akter/ns_main/dataset/devign_test.csv')\n",
    "CV_test_df.info()"
   ],
   "id": "f4bbada4a1afcc90"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T13:02:58.856401Z",
     "start_time": "2025-06-30T13:02:58.784466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "CV_test_df = pd.read_csv(\"devign_test.csv\")\n",
    "CV_test_df.info()"
   ],
   "id": "a83434df26c2fc4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 900 entries, 0 to 899\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   900 non-null    object\n",
      " 1   cwe_id  900 non-null    object\n",
      " 2   output  900 non-null    int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 21.2+ KB\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T13:03:25.229195Z",
     "start_time": "2025-06-30T13:03:00.748047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = \"./unixcoder_nasa_finetuned\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "CV_test_df= CV_test_df[CV_test_df[\"output\"].isin([0, 1])]\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "CV_test_df = CV_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "CV_test_df = CV_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "\n",
    "CV_test_ds = Dataset.from_pandas(CV_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]] ,\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "CV_test_tok = CV_test_ds.map(tokenize_fn, batched=True)\n",
    "CV_test_tok = CV_test_tok.rename_column(\"label\", \"labels\")\n",
    "CV_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=CV_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on RVL Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "62e4adeda36aa1ed",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [00:00<00:00, 4677.77 examples/s]\n",
      "/tmp/ipykernel_669099/4291597000.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:03]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on RVL Dataset ===\n",
      "loss        : 5.7414\n",
      "model_preparation_time: 0.0017\n",
      "accuracy    : 0.5000\n",
      "precision   : 0.5000\n",
      "recall      : 1.0000\n",
      "specificity : 0.0000\n",
      "fpr         : 1.0000\n",
      "f1          : 0.6667\n",
      "mcc         : 0.0000\n",
      "kappa       : 0.0000\n",
      "mse         : 0.5000\n",
      "mae         : 0.5000\n",
      "auc         : 0.5078\n",
      "runtime     : 4.0806\n",
      "samples_per_second: 147.0380\n",
      "steps_per_second: 18.3800\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T13:05:39.120855Z",
     "start_time": "2025-06-30T13:05:39.094112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "div_test_df = pd.read_csv(\"diverse_test.csv\")\n",
    "div_test_df.info()"
   ],
   "id": "15c3333cce41e4ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1532 entries, 0 to 1531\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   code_snip  1532 non-null   object\n",
      " 1   output     1532 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.1+ KB\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T13:06:09.394597Z",
     "start_time": "2025-06-30T13:05:41.552841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = \"./unixcoder_nasa_finetuned\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "div_test_df= div_test_df[div_test_df[\"output\"].isin([0, 1])]\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "div_test_df = div_test_df.rename(columns={\"code_snip\": \"func\", \"output\": \"label\"})\n",
    "div_test_df = div_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "\n",
    "div_test_ds = Dataset.from_pandas(div_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]] ,\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "div_test_tok = div_test_ds.map(tokenize_fn, batched=True)\n",
    "div_test_tok = div_test_tok.rename_column(\"label\", \"labels\")\n",
    "div_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=div_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on RVL Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "99f8a0381072fa21",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1532/1532 [00:00<00:00, 6688.12 examples/s]\n",
      "/tmp/ipykernel_669099/3034093374.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [192/192 00:09]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on RVL Dataset ===\n",
      "loss        : 5.7048\n",
      "model_preparation_time: 0.0018\n",
      "accuracy    : 0.5000\n",
      "precision   : 0.5000\n",
      "recall      : 1.0000\n",
      "specificity : 0.0000\n",
      "fpr         : 1.0000\n",
      "f1          : 0.6667\n",
      "mcc         : 0.0000\n",
      "kappa       : 0.0000\n",
      "mse         : 0.5000\n",
      "mae         : 0.5000\n",
      "auc         : 0.5892\n",
      "runtime     : 9.6280\n",
      "samples_per_second: 159.1190\n",
      "steps_per_second: 19.9420\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T13:06:35.660336Z",
     "start_time": "2025-06-30T13:06:35.627019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "dj_test_df = pd.read_csv(\"djuliet_test.csv\")\n",
    "dj_test_df.info()"
   ],
   "id": "255c0c0d5a405aba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3152 entries, 0 to 3151\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   code_snip  3152 non-null   object\n",
      " 1   output     3152 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 49.4+ KB\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T13:07:18.201119Z",
     "start_time": "2025-06-30T13:06:40.610096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = \"./unixcoder_nasa_finetuned\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "dj_test_df= dj_test_df[dj_test_df[\"output\"].isin([0, 1])]\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "dj_test_df = dj_test_df.rename(columns={\"code_snip\": \"func\", \"output\": \"label\"})\n",
    "dj_test_df = dj_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "\n",
    "dj_test_ds = Dataset.from_pandas(dj_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]] ,\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "dj_test_tok = dj_test_ds.map(tokenize_fn, batched=True)\n",
    "dj_test_tok = dj_test_tok.rename_column(\"label\", \"labels\")\n",
    "dj_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=dj_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on RVL Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "dd35cd0f53c01e52",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3152/3152 [00:00<00:00, 10630.46 examples/s]\n",
      "/tmp/ipykernel_669099/872764684.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='394' max='394' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [394/394 00:20]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on RVL Dataset ===\n",
      "loss        : 5.8866\n",
      "model_preparation_time: 0.0019\n",
      "accuracy    : 0.5000\n",
      "precision   : 0.5000\n",
      "recall      : 1.0000\n",
      "specificity : 0.0000\n",
      "fpr         : 1.0000\n",
      "f1          : 0.6667\n",
      "mcc         : 0.0000\n",
      "kappa       : 0.0000\n",
      "mse         : 0.5000\n",
      "mae         : 0.5000\n",
      "auc         : 0.4782\n",
      "runtime     : 20.3466\n",
      "samples_per_second: 154.9150\n",
      "steps_per_second: 19.3640\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T13:08:30.944917Z",
     "start_time": "2025-06-30T13:08:30.919723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "big_test_df = pd.read_csv(\"big_vultest.csv\")\n",
    "big_test_df.info()"
   ],
   "id": "8899fa56f65fed12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1170 entries, 0 to 1169\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  1170 non-null   object\n",
      " 1   input        1170 non-null   object\n",
      " 2   output       1170 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 27.5+ KB\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T13:08:58.394494Z",
     "start_time": "2025-06-30T13:08:32.739143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = \"./unixcoder_nasa_finetuned\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "big_test_df= big_test_df[big_test_df[\"output\"].isin([0, 1])]\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "big_test_df = big_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "big_test_df = big_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "\n",
    "big_test_ds = Dataset.from_pandas(big_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]] ,\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "big_test_tok = big_test_ds.map(tokenize_fn, batched=True)\n",
    "big_test_tok = big_test_tok.rename_column(\"label\", \"labels\")\n",
    "big_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=big_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on RVL Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "23a269f9c7e8080d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1170/1170 [00:00<00:00, 9158.48 examples/s]\n",
      "/tmp/ipykernel_669099/2848276077.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [147/147 00:07]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on RVL Dataset ===\n",
      "loss        : 5.5969\n",
      "model_preparation_time: 0.0019\n",
      "accuracy    : 0.5000\n",
      "precision   : 0.5000\n",
      "recall      : 1.0000\n",
      "specificity : 0.0000\n",
      "fpr         : 1.0000\n",
      "f1          : 0.6667\n",
      "mcc         : 0.0000\n",
      "kappa       : 0.0000\n",
      "mse         : 0.5000\n",
      "mae         : 0.5000\n",
      "auc         : 0.6008\n",
      "runtime     : 7.3537\n",
      "samples_per_second: 159.1040\n",
      "steps_per_second: 19.9900\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T13:09:35.690113Z",
     "start_time": "2025-06-30T13:09:35.645909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cve_test_df = pd.read_json(\"test_512.json\")\n",
    "cve_test_df.info()"
   ],
   "id": "cbf31ba26d77f089",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4216 entries, 0 to 4215\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  4216 non-null   object\n",
      " 1   input        4216 non-null   object\n",
      " 2   output       4216 non-null   int64 \n",
      " 3   idx          4216 non-null   int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 131.9+ KB\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T13:10:24.138175Z",
     "start_time": "2025-06-30T13:09:39.737625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = \"./unixcoder_nasa_finetuned\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "cve_test_df= cve_test_df[cve_test_df[\"output\"].isin([0, 1])]\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "cve_test_df = cve_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "cve_test_df = cve_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "\n",
    "cve_test_ds = Dataset.from_pandas(cve_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]] ,\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "cve_test_tok = cve_test_ds.map(tokenize_fn, batched=True)\n",
    "cve_test_tok = cve_test_tok.rename_column(\"label\", \"labels\")\n",
    "cve_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=cve_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on RVL Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "618f4b4183da04a7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4216/4216 [00:00<00:00, 9772.30 examples/s] \n",
      "/tmp/ipykernel_669099/238134083.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='527' max='527' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [527/527 00:25]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on RVL Dataset ===\n",
      "loss        : 5.7907\n",
      "model_preparation_time: 0.0019\n",
      "accuracy    : 0.5000\n",
      "precision   : 0.5000\n",
      "recall      : 1.0000\n",
      "specificity : 0.0000\n",
      "fpr         : 1.0000\n",
      "f1          : 0.6667\n",
      "mcc         : 0.0000\n",
      "kappa       : 0.0000\n",
      "mse         : 0.5000\n",
      "mae         : 0.5000\n",
      "auc         : 0.5163\n",
      "runtime     : 26.0298\n",
      "samples_per_second: 161.9680\n",
      "steps_per_second: 20.2460\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
