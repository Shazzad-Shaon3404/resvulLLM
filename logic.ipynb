{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-18T15:18:07.170237Z",
     "start_time": "2025-07-18T15:18:05.728757Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('train_fol.csv')\n",
    "test_df = pd.read_csv('test_fol.csv')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Devign Dataset with Codebert FOL",
   "id": "37c03ae4dba6b7d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T23:06:07.496704Z",
     "start_time": "2025-05-26T16:00:52.928422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== STEP 1: Data Preprocessing ======\n",
    "# Rename to standardized column names\n",
    "train_df = train_df.rename(columns={\n",
    "    \"func_cleaned\": \"func\",\n",
    "    \"target\": \"label\",\n",
    "    \"fol_logic\": \"fol\"\n",
    "})\n",
    "test_df = test_df.rename(columns={\n",
    "    \"func_cleaned\": \"func\",\n",
    "    \"target\": \"label\"\n",
    "})\n",
    "\n",
    "# Drop rows with missing values\n",
    "train_df = train_df.dropna(subset=[\"fol\", \"func\", \"label\"])\n",
    "test_df = test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "train_df[\"label\"] = train_df[\"label\"].astype(int)\n",
    "test_df[\"label\"] = test_df[\"label\"].astype(int)\n",
    "\n",
    "# Convert to HuggingFace Datasets\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds  = Dataset.from_pandas(test_df)\n",
    "\n",
    "# ====== STEP 2: Tokenizer Setup ======\n",
    "model_name = \"microsoft/codebert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Train: use FOL + FUNC\n",
    "def tokenize_fn_train(batch):\n",
    "    return tokenizer(\n",
    "        [\"FOL: \" + f + \" FUNC: \" + c for f, c in zip(batch[\"fol\"], batch[\"func\"])],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "# Test: use FUNC only\n",
    "def tokenize_fn_test(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "# Apply tokenizer\n",
    "train_tok = train_ds.map(tokenize_fn_train, batched=True)\n",
    "test_tok  = test_ds.map(tokenize_fn_test, batched=True)\n",
    "\n",
    "# ====== STEP 3: Prepare datasets ======\n",
    "train_tok = train_tok.rename_column('label', 'labels')\n",
    "test_tok  = test_tok.rename_column('label', 'labels')\n",
    "train_tok.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_tok.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# ====== STEP 4: Load Model ======\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.config.problem_type = \"single_label_classification\"\n",
    "model.config.id2label = {0: \"safe\", 1: \"vuln\"}\n",
    "model.config.label2id = {\"safe\": 0, \"vuln\": 1}\n",
    "\n",
    "# ====== STEP 5: Metrics ======\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "# ====== STEP 6: Training Setup ======\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "args = TrainingArguments(\n",
    "    output_dir='./codebert_trainFOL_testFUNC',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=test_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "\n",
    "# ====== STEP 7: Train ======\n",
    "start_train = time.time()\n",
    "trainer.train()\n",
    "end_train = time.time()\n",
    "train_runtime = end_train - start_train\n",
    "print(f\"\\n=== Training completed in {train_runtime:.2f} seconds ({train_runtime / 60:.2f} minutes) ===\")\n",
    "\n",
    "# ====== STEP 8: Evaluate ======\n",
    "start_eval = time.time()\n",
    "metrics = trainer.evaluate()\n",
    "end_eval = time.time()\n",
    "eval_runtime = end_eval - start_eval\n",
    "print(f\"\\n=== Evaluation completed in {eval_runtime:.2f} seconds ({eval_runtime / 60:.2f} minutes) ===\")\n",
    "\n",
    "print(\"\\n=== Hold‐out Metrics (Trained with FOL + FUNC, Tested with FUNC only) ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n",
    "\n",
    "# Include runtime in printed metrics\n",
    "print(f\"Train Time (s): {train_runtime:.2f}\")\n",
    "print(f\"Eval Time (s):  {eval_runtime:.2f}\")\n",
    "\n",
    "# ====== STEP 9: Save Model ======\n",
    "save_dir = './codebert_trainFOL_testFUNC_saved'\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(f\"\\nModel and tokenizer saved to {save_dir}\")\n"
   ],
   "id": "44e77b2011bf9d74",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7952/7952 [00:06<00:00, 1140.60 examples/s]\n",
      "Map: 100%|██████████| 2807/2807 [00:02<00:00, 1333.29 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_140776\\947603458.py:120: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2982' max='4970' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2982/4970 6:52:56 < 4:35:28, 0.12 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Fpr</th>\n",
       "      <th>F1</th>\n",
       "      <th>Mcc</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.685200</td>\n",
       "      <td>0.679471</td>\n",
       "      <td>0.566797</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.033717</td>\n",
       "      <td>0.974230</td>\n",
       "      <td>0.025770</td>\n",
       "      <td>0.063174</td>\n",
       "      <td>0.023384</td>\n",
       "      <td>0.008928</td>\n",
       "      <td>0.433203</td>\n",
       "      <td>0.433203</td>\n",
       "      <td>0.554645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.682700</td>\n",
       "      <td>0.676147</td>\n",
       "      <td>0.589954</td>\n",
       "      <td>0.527197</td>\n",
       "      <td>0.518092</td>\n",
       "      <td>0.644877</td>\n",
       "      <td>0.355123</td>\n",
       "      <td>0.522605</td>\n",
       "      <td>0.163321</td>\n",
       "      <td>0.163302</td>\n",
       "      <td>0.410046</td>\n",
       "      <td>0.410046</td>\n",
       "      <td>0.625034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.537200</td>\n",
       "      <td>0.772918</td>\n",
       "      <td>0.619523</td>\n",
       "      <td>0.596859</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.806411</td>\n",
       "      <td>0.193589</td>\n",
       "      <td>0.460606</td>\n",
       "      <td>0.201969</td>\n",
       "      <td>0.189722</td>\n",
       "      <td>0.380477</td>\n",
       "      <td>0.380477</td>\n",
       "      <td>0.654622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training completed in 24792.31 seconds (413.21 minutes) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [351/351 11:49]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation completed in 710.90 seconds (11.85 minutes) ===\n",
      "\n",
      "=== Hold‐out Metrics (Trained with FOL + FUNC, Tested with FUNC only) ===\n",
      "loss        : 0.6761\n",
      "accuracy    : 0.5900\n",
      "precision   : 0.5272\n",
      "recall      : 0.5181\n",
      "specificity : 0.6449\n",
      "fpr         : 0.3551\n",
      "f1          : 0.5226\n",
      "mcc         : 0.1633\n",
      "kappa       : 0.1633\n",
      "mse         : 0.4100\n",
      "mae         : 0.4100\n",
      "auc         : 0.6250\n",
      "runtime     : 710.8814\n",
      "samples_per_second: 3.9490\n",
      "steps_per_second: 0.4940\n",
      "Train Time (s): 24792.31\n",
      "Eval Time (s):  710.90\n",
      "\n",
      "Model and tokenizer saved to ./codebert_trainFOL_testFUNC_saved\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "reveal with codebetrt FOL",
   "id": "66c02235a2273885"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T14:37:21.419256Z",
     "start_time": "2025-05-31T14:37:21.350206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "reveal_test_df = pd.read_csv(\"Reveal_vultest.csv\")\n",
    "reveal_test_df.info()"
   ],
   "id": "450c90d9fe73be59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2028 entries, 0 to 2027\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  2028 non-null   object\n",
      " 1   input        2028 non-null   object\n",
      " 2   output       2028 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 47.7+ KB\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T03:35:59.705050Z",
     "start_time": "2025-05-29T03:21:44.717770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './codebert_trainFOL_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "reveal_test_df = pd.read_csv(\"Reveal_vultest.csv\")\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "reveal_test_df = reveal_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "reveal_test_df = reveal_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "reveal_test_ds = Dataset.from_pandas(reveal_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "reveal_test_tok = reveal_test_ds.map(tokenize_fn, batched=True)\n",
    "reveal_test_tok = reveal_test_tok.rename_column(\"label\", \"labels\")\n",
    "reveal_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=reveal_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on Reveal Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "fda8c495cf5a81b4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2028/2028 [00:00<00:00, 2350.06 examples/s]\n",
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_37716\\3486837485.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='254' max='254' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [254/254 14:08]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on Reveal Dataset ===\n",
      "loss        : 0.6953\n",
      "model_preparation_time: 0.0074\n",
      "accuracy    : 0.5394\n",
      "precision   : 0.5512\n",
      "recall      : 0.4250\n",
      "specificity : 0.6538\n",
      "fpr         : 0.3462\n",
      "f1          : 0.4800\n",
      "mcc         : 0.0810\n",
      "kappa       : 0.0789\n",
      "mse         : 0.4606\n",
      "mae         : 0.4606\n",
      "auc         : 0.5722\n",
      "runtime     : 853.5330\n",
      "samples_per_second: 2.3760\n",
      "steps_per_second: 0.2980\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "big_vultest   with codebert FOL",
   "id": "9a81238dfce28290"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T14:41:42.630237Z",
     "start_time": "2025-05-31T14:41:42.582062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "big_test_df = pd.read_csv(\"big_vultest.csv\")\n",
    "big_test_df.info()"
   ],
   "id": "587e83e0d0b418ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1170 entries, 0 to 1169\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  1170 non-null   object\n",
      " 1   input        1170 non-null   object\n",
      " 2   output       1170 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 27.5+ KB\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T04:43:37.423040Z",
     "start_time": "2025-05-29T04:38:49.336844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './codebert_trainFOL_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "big_test_df = big_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "big_test_df = big_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "big_test_ds = Dataset.from_pandas(big_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "big_test_tok = big_test_ds.map(tokenize_fn, batched=True)\n",
    "big_test_tok = big_test_tok.rename_column(\"label\", \"labels\")\n",
    "big_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=big_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on Big Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "e3f8427b55950f89",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1170/1170 [00:00<00:00, 4417.49 examples/s]\n",
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_37716\\3834133774.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [147/147 04:44]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on Big Dataset ===\n",
      "loss        : 0.7203\n",
      "model_preparation_time: 0.0020\n",
      "accuracy    : 0.4991\n",
      "precision   : 0.4992\n",
      "recall      : 0.5385\n",
      "specificity : 0.4598\n",
      "fpr         : 0.5402\n",
      "f1          : 0.5181\n",
      "mcc         : -0.0017\n",
      "kappa       : -0.0017\n",
      "mse         : 0.5009\n",
      "mae         : 0.5009\n",
      "auc         : 0.4866\n",
      "runtime     : 287.4263\n",
      "samples_per_second: 4.0710\n",
      "steps_per_second: 0.5110\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Diverse_test with codebert FOL",
   "id": "906e5caa7ddcd977"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T04:33:32.446284Z",
     "start_time": "2025-05-31T04:33:32.415007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "diverse_test_df = pd.read_csv(\"diverse_test.csv\")\n",
    "diverse_test_df.info()"
   ],
   "id": "b4b41a2329e94d32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1532 entries, 0 to 1531\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   code_snip  1532 non-null   object\n",
      " 1   output     1532 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.1+ KB\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T04:50:12.258150Z",
     "start_time": "2025-05-29T04:44:00.257030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './codebert_trainFOL_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "diverse_test_df = diverse_test_df.rename(columns={\"code_snip\": \"func\", \"output\": \"label\"})\n",
    "diverse_test_df = diverse_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "diverse_test_ds = Dataset.from_pandas(diverse_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "diverse_test_tok = diverse_test_ds.map(tokenize_fn, batched=True)\n",
    "diverse_test_tok = diverse_test_tok.rename_column(\"label\", \"labels\")\n",
    "diverse_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=diverse_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on diverse Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "ce44ca1c01485b4f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1532/1532 [00:00<00:00, 4822.60 examples/s]\n",
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_37716\\2459428948.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [192/192 06:08]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on diverse Dataset ===\n",
      "loss        : 0.7060\n",
      "model_preparation_time: 0.0063\n",
      "accuracy    : 0.5274\n",
      "precision   : 0.5233\n",
      "recall      : 0.6149\n",
      "specificity : 0.4399\n",
      "fpr         : 0.5601\n",
      "f1          : 0.5654\n",
      "mcc         : 0.0557\n",
      "kappa       : 0.0548\n",
      "mse         : 0.4726\n",
      "mae         : 0.4726\n",
      "auc         : 0.5273\n",
      "runtime     : 371.3610\n",
      "samples_per_second: 4.1250\n",
      "steps_per_second: 0.5170\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "djuliet _test with codebert fol",
   "id": "3309ecb09c5758"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T04:01:04.924820Z",
     "start_time": "2025-05-31T04:01:04.861401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "juliet_test_df = pd.read_csv(\"djuliet_test.csv\")\n",
    "juliet_test_df.info()"
   ],
   "id": "3bbf7819175d4b6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3152 entries, 0 to 3151\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   code_snip  3152 non-null   object\n",
      " 1   output     3152 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 49.4+ KB\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T05:11:13.903247Z",
     "start_time": "2025-05-29T04:58:25.481093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './codebert_trainFOL_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "juliet_test_df = juliet_test_df.rename(columns={\"code_snip\": \"func\", \"output\": \"label\"})\n",
    "juliet_test_df = juliet_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "juliet_test_ds = Dataset.from_pandas(juliet_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "juliet_test_tok = juliet_test_ds.map(tokenize_fn, batched=True)\n",
    "juliet_test_tok = juliet_test_tok.rename_column(\"label\", \"labels\")\n",
    "juliet_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=juliet_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on Juliet Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "bdba92c2043fd16",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3152/3152 [00:00<00:00, 4749.74 examples/s]\n",
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_37716\\2231731310.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='394' max='394' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [394/394 12:45]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on Juliet Dataset ===\n",
      "loss        : 0.6963\n",
      "model_preparation_time: 0.0040\n",
      "accuracy    : 0.5235\n",
      "precision   : 0.5303\n",
      "recall      : 0.4105\n",
      "specificity : 0.6364\n",
      "fpr         : 0.3636\n",
      "f1          : 0.4628\n",
      "mcc         : 0.0482\n",
      "kappa       : 0.0470\n",
      "mse         : 0.4765\n",
      "mae         : 0.4765\n",
      "auc         : 0.5368\n",
      "runtime     : 767.3802\n",
      "samples_per_second: 4.1070\n",
      "steps_per_second: 0.5130\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "mixvultest-codebert FOL",
   "id": "7bce6f0c1fa2e8d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T04:28:52.911445Z",
     "start_time": "2025-05-31T04:28:52.842444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "mix_test_df = pd.read_csv(\"mix_test_vultest.csv\")\n",
    "mix_test_df.info()"
   ],
   "id": "919e165ef33ad63e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2864 entries, 0 to 2863\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  2864 non-null   object\n",
      " 1   input        2864 non-null   object\n",
      " 2   output       2864 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 67.2+ KB\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T17:24:03.773154Z",
     "start_time": "2025-05-29T17:05:21.612430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './codebert_trainFOL_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "mix_test_df = mix_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "mix_test_df = mix_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "mix_test_ds = Dataset.from_pandas(mix_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "mix_test_tok = mix_test_ds.map(tokenize_fn, batched=True)\n",
    "mix_test_tok = mix_test_tok.rename_column(\"label\", \"labels\")\n",
    "mix_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=mix_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on mix Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "7f0f06eb1f8a5b80",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|██████████| 2864/2864 [00:00<00:00, 3261.87 examples/s]\n",
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_10548\\861938124.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='358' max='358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [358/358 17:44]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on mix Dataset ===\n",
      "loss        : 0.7039\n",
      "model_preparation_time: 0.0104\n",
      "accuracy    : 0.5230\n",
      "precision   : 0.5226\n",
      "recall      : 0.5335\n",
      "specificity : 0.5126\n",
      "fpr         : 0.4874\n",
      "f1          : 0.5280\n",
      "mcc         : 0.0461\n",
      "kappa       : 0.0461\n",
      "mse         : 0.4770\n",
      "mae         : 0.4770\n",
      "auc         : 0.5391\n",
      "runtime     : 1068.5041\n",
      "samples_per_second: 2.6800\n",
      "steps_per_second: 0.3350\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cve fixes FOL Cb",
   "id": "eb18023c03540537"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T18:06:21.608852Z",
     "start_time": "2025-05-29T18:06:21.561335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "fun_test_df = pd.read_json(\"test_512.json\")\n",
    "fun_test_df.info()"
   ],
   "id": "583e779d95194116",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4216 entries, 0 to 4215\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  4216 non-null   object\n",
      " 1   input        4216 non-null   object\n",
      " 2   output       4216 non-null   int64 \n",
      " 3   idx          4216 non-null   int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 131.9+ KB\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T18:25:25.384467Z",
     "start_time": "2025-05-29T18:06:26.511020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './codebert_trainFOL_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "fun_test_df = fun_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "fun_test_df = fun_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "fun_test_ds = Dataset.from_pandas(fun_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "fun_test_tok = fun_test_ds.map(tokenize_fn, batched=True)\n",
    "fun_test_tok = fun_test_tok.rename_column(\"label\", \"labels\")\n",
    "fun_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=fun_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on VULLM Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "220cc709408c5754",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4216/4216 [00:00<00:00, 4559.29 examples/s]\n",
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_10548\\703939202.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='527' max='527' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [527/527 18:55]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on VULLM Dataset ===\n",
      "loss        : 0.7185\n",
      "model_preparation_time: 0.0000\n",
      "accuracy    : 0.4848\n",
      "precision   : 0.4890\n",
      "recall      : 0.6731\n",
      "specificity : 0.2965\n",
      "fpr         : 0.7035\n",
      "f1          : 0.5665\n",
      "mcc         : -0.0328\n",
      "kappa       : -0.0304\n",
      "mse         : 0.5152\n",
      "mae         : 0.5152\n",
      "auc         : 0.4811\n",
      "runtime     : 1137.6196\n",
      "samples_per_second: 3.7060\n",
      "steps_per_second: 0.4630\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---------------------------------------------------------------------------",
   "id": "bcbb11a2dca7cf8a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ALL codebert",
   "id": "285d6f7882d5f9e9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "NOW-----WITHOUT--------------------FOl",
   "id": "91fb436ff627b4db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Devign",
   "id": "92208d3ce00369fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T15:43:20.652049Z",
     "start_time": "2025-07-18T15:18:34.915969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== STEP 1: Load Data ======\n",
    "train_df = pd.read_csv('train_fol.csv')\n",
    "test_df = pd.read_csv('test_fol.csv')\n",
    "\n",
    "# Rename to standardized column names\n",
    "train_df = train_df.rename(columns={\n",
    "    \"func_cleaned\": \"func\",\n",
    "    \"target\": \"label\"\n",
    "})\n",
    "test_df = test_df.rename(columns={\n",
    "    \"func_cleaned\": \"func\",\n",
    "    \"target\": \"label\"\n",
    "})\n",
    "\n",
    "# Drop rows with missing values\n",
    "train_df = train_df.dropna(subset=[\"func\", \"label\"])\n",
    "test_df = test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "train_df[\"label\"] = train_df[\"label\"].astype(int)\n",
    "test_df[\"label\"] = test_df[\"label\"].astype(int)\n",
    "\n",
    "# Convert to HuggingFace Datasets\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds  = Dataset.from_pandas(test_df)\n",
    "\n",
    "# ====== STEP 2: Tokenizer Setup ======\n",
    "model_name = \"microsoft/codebert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "# Apply tokenizer\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True)\n",
    "test_tok  = test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "# ====== STEP 3: Prepare datasets ======\n",
    "train_tok = train_tok.rename_column('label', 'labels')\n",
    "test_tok  = test_tok.rename_column('label', 'labels')\n",
    "train_tok.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_tok.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# ====== STEP 4: Load Model ======\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.config.problem_type = \"single_label_classification\"\n",
    "model.config.id2label = {0: \"safe\", 1: \"vuln\"}\n",
    "model.config.label2id = {\"safe\": 0, \"vuln\": 1}\n",
    "\n",
    "# ====== STEP 5: Metrics ======\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "# ====== STEP 6: Training Setup ======\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "args = TrainingArguments(\n",
    "    output_dir='./codebert_trainFUNC_testFUNC',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=test_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "\n",
    "# ====== STEP 7: Train ======\n",
    "start_train = time.time()\n",
    "trainer.train()\n",
    "end_train = time.time()\n",
    "train_runtime = end_train - start_train\n",
    "print(f\"\\n=== Training completed in {train_runtime:.2f} seconds ({train_runtime / 60:.2f} minutes) ===\")\n",
    "\n",
    "# ====== STEP 8: Evaluate ======\n",
    "start_eval = time.time()\n",
    "metrics = trainer.evaluate()\n",
    "end_eval = time.time()\n",
    "eval_runtime = end_eval - start_eval\n",
    "print(f\"\\n=== Evaluation completed in {eval_runtime:.2f} seconds ({eval_runtime / 60:.2f} minutes) ===\")\n",
    "\n",
    "print(\"\\n=== Hold-out Metrics (Trained and Tested with FUNC only) ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n",
    "\n",
    "print(f\"Train Time (s): {train_runtime:.2f}\")\n",
    "print(f\"Eval Time (s):  {eval_runtime:.2f}\")\n",
    "\n",
    "# ====== STEP 9: Save Model ======\n",
    "save_dir = './codebert_trainFUNC_testFUNC_saved'\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(f\"\\nModel and tokenizer saved to {save_dir}\")\n"
   ],
   "id": "1112142323b9cf1b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\PycharmProjects\\logical_llm\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user01\\PycharmProjects\\logical_llm\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 11225/11225 [00:02<00:00, 4313.46 examples/s]\n",
      "Map: 100%|██████████| 2807/2807 [00:00<00:00, 5546.03 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_40492\\888024945.py:112: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7020' max='7020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7020/7020 23:51, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Fpr</th>\n",
       "      <th>F1</th>\n",
       "      <th>Mcc</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.684246</td>\n",
       "      <td>0.566797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433203</td>\n",
       "      <td>0.433203</td>\n",
       "      <td>0.519235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.682200</td>\n",
       "      <td>0.676593</td>\n",
       "      <td>0.574635</td>\n",
       "      <td>0.537415</td>\n",
       "      <td>0.129934</td>\n",
       "      <td>0.914519</td>\n",
       "      <td>0.085481</td>\n",
       "      <td>0.209272</td>\n",
       "      <td>0.071934</td>\n",
       "      <td>0.048815</td>\n",
       "      <td>0.425365</td>\n",
       "      <td>0.425365</td>\n",
       "      <td>0.605471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.603500</td>\n",
       "      <td>0.632615</td>\n",
       "      <td>0.627716</td>\n",
       "      <td>0.577376</td>\n",
       "      <td>0.524671</td>\n",
       "      <td>0.706474</td>\n",
       "      <td>0.293526</td>\n",
       "      <td>0.549763</td>\n",
       "      <td>0.234437</td>\n",
       "      <td>0.233659</td>\n",
       "      <td>0.372284</td>\n",
       "      <td>0.372284</td>\n",
       "      <td>0.684019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.501100</td>\n",
       "      <td>0.679565</td>\n",
       "      <td>0.650517</td>\n",
       "      <td>0.620266</td>\n",
       "      <td>0.498355</td>\n",
       "      <td>0.766813</td>\n",
       "      <td>0.233187</td>\n",
       "      <td>0.552668</td>\n",
       "      <td>0.275836</td>\n",
       "      <td>0.271456</td>\n",
       "      <td>0.349483</td>\n",
       "      <td>0.349483</td>\n",
       "      <td>0.708149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.452300</td>\n",
       "      <td>0.683632</td>\n",
       "      <td>0.652654</td>\n",
       "      <td>0.618954</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.757385</td>\n",
       "      <td>0.242615</td>\n",
       "      <td>0.562584</td>\n",
       "      <td>0.281686</td>\n",
       "      <td>0.278489</td>\n",
       "      <td>0.347346</td>\n",
       "      <td>0.347346</td>\n",
       "      <td>0.715717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training completed in 1433.05 seconds (23.88 minutes) ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [351/351 00:17]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation completed in 17.81 seconds (0.30 minutes) ===\n",
      "\n",
      "=== Hold-out Metrics (Trained and Tested with FUNC only) ===\n",
      "loss        : 0.6836\n",
      "accuracy    : 0.6527\n",
      "precision   : 0.6190\n",
      "recall      : 0.5156\n",
      "specificity : 0.7574\n",
      "fpr         : 0.2426\n",
      "f1          : 0.5626\n",
      "mcc         : 0.2817\n",
      "kappa       : 0.2785\n",
      "mse         : 0.3473\n",
      "mae         : 0.3473\n",
      "auc         : 0.7157\n",
      "runtime     : 17.7896\n",
      "samples_per_second: 157.7890\n",
      "steps_per_second: 19.7310\n",
      "Train Time (s): 1433.05\n",
      "Eval Time (s):  17.81\n",
      "\n",
      "Model and tokenizer saved to ./codebert_trainFUNC_testFUNC_saved\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T15:06:50.927908Z",
     "start_time": "2025-05-30T15:06:50.769218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "D_test_df = pd.read_csv('test_fol.csv')\n",
    "D_test_df.info()"
   ],
   "id": "dcfb2e3eb9483ed7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2807 entries, 0 to 2806\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   func_cleaned   2807 non-null   object\n",
      " 1   target         2807 non-null   bool  \n",
      " 2   fol_logic      1934 non-null   object\n",
      " 3   combined_code  2807 non-null   object\n",
      "dtypes: bool(1), object(3)\n",
      "memory usage: 68.7+ KB\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T15:53:15.187144Z",
     "start_time": "2025-05-30T15:41:27.143758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './codebert_trainFUNC_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "D_test_df = D_test_df.rename(columns={\"func_cleaned\": \"func\", \"target\": \"label\"})\n",
    "D_test_df = D_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "D_test_df[\"label\"] = D_test_df[\"label\"].astype(int)\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "D_test_ds = Dataset.from_pandas(D_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "D_test_tok = D_test_ds.map(tokenize_fn, batched=True)\n",
    "D_test_tok = D_test_tok.rename_column(\"label\", \"labels\")\n",
    "D_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=D_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on Devign withot Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "9aec5517043961f8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2807/2807 [00:01<00:00, 2418.67 examples/s]\n",
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_10548\\45701465.py:74: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [351/351 11:44]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on Devign withot Dataset ===\n",
      "loss        : 0.6610\n",
      "model_preparation_time: 0.0000\n",
      "accuracy    : 0.6676\n",
      "precision   : 0.6206\n",
      "recall      : 0.5987\n",
      "specificity : 0.7203\n",
      "fpr         : 0.2797\n",
      "f1          : 0.6095\n",
      "mcc         : 0.3205\n",
      "kappa       : 0.3203\n",
      "mse         : 0.3324\n",
      "mae         : 0.3324\n",
      "auc         : 0.7322\n",
      "runtime     : 706.5708\n",
      "samples_per_second: 3.9730\n",
      "steps_per_second: 0.4970\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "REVEAL without FOL CB",
   "id": "7f5c3ea61bee9ea7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T13:07:15.721076Z",
     "start_time": "2025-05-30T13:07:15.649293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "Wreveal_test_df = pd.read_csv(\"Reveal_vultest.csv\")\n",
    "Wreveal_test_df.info()"
   ],
   "id": "e640d0b292613275",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2028 entries, 0 to 2027\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  2028 non-null   object\n",
      " 1   input        2028 non-null   object\n",
      " 2   output       2028 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 47.7+ KB\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T13:16:41.348908Z",
     "start_time": "2025-05-30T13:07:19.343413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './codebert_trainFUNC_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "Wreveal_test_df = pd.read_csv(\"Reveal_vultest.csv\")\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "Wreveal_test_df = Wreveal_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "Wreveal_test_df = Wreveal_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "Wreveal_test_ds = Dataset.from_pandas(Wreveal_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "Wreveal_test_tok = Wreveal_test_ds.map(tokenize_fn, batched=True)\n",
    "Wreveal_test_tok = Wreveal_test_tok.rename_column(\"label\", \"labels\")\n",
    "Wreveal_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=Wreveal_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on Reveal without Fol Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "c53202f4c17a5b1b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2028/2028 [00:00<00:00, 3285.00 examples/s]\n",
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_10548\\2154652566.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='254' max='254' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [254/254 09:18]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on Reveal without Fol Dataset ===\n",
      "loss        : 2.0654\n",
      "model_preparation_time: 0.0000\n",
      "accuracy    : 0.5005\n",
      "precision   : 0.5003\n",
      "recall      : 0.8688\n",
      "specificity : 0.1321\n",
      "fpr         : 0.8679\n",
      "f1          : 0.6350\n",
      "mcc         : 0.0015\n",
      "kappa       : 0.0010\n",
      "mse         : 0.4995\n",
      "mae         : 0.4995\n",
      "auc         : 0.4625\n",
      "runtime     : 560.7258\n",
      "samples_per_second: 3.6170\n",
      "steps_per_second: 0.4530\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "bigvul without FOl Cb",
   "id": "7873d5e6a883bc12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T14:43:21.179067Z",
     "start_time": "2025-05-31T14:43:21.149323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "Wbig_test_df = pd.read_csv(\"big_vultest.csv\")\n",
    "Wbig_test_df.info()"
   ],
   "id": "5c4d53e93253f473",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1170 entries, 0 to 1169\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  1170 non-null   object\n",
      " 1   input        1170 non-null   object\n",
      " 2   output       1170 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 27.5+ KB\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T15:38:34.186363Z",
     "start_time": "2025-05-30T15:32:03.927198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './codebert_trainFUNC_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "Wbig_test_df = Wbig_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "Wbig_test_df = Wbig_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "Wbig_test_ds = Dataset.from_pandas(Wbig_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "Wbig_test_tok = Wbig_test_ds.map(tokenize_fn, batched=True)\n",
    "Wbig_test_tok = Wbig_test_tok.rename_column(\"label\", \"labels\")\n",
    "Wbig_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=Wbig_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on Big without Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "f337e5db75b899d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1170/1170 [00:00<00:00, 3694.26 examples/s]\n",
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_10548\\4287014798.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [147/147 06:27]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on Big without Dataset ===\n",
      "loss        : 2.1061\n",
      "model_preparation_time: 0.0068\n",
      "accuracy    : 0.4812\n",
      "precision   : 0.4893\n",
      "recall      : 0.8615\n",
      "specificity : 0.1009\n",
      "fpr         : 0.8991\n",
      "f1          : 0.6241\n",
      "mcc         : -0.0579\n",
      "kappa       : -0.0376\n",
      "mse         : 0.5188\n",
      "mae         : 0.5188\n",
      "auc         : 0.3845\n",
      "runtime     : 389.3552\n",
      "samples_per_second: 3.0050\n",
      "steps_per_second: 0.3780\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Diverse without CB",
   "id": "b003069b1d32514e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T14:43:31.784994Z",
     "start_time": "2025-05-31T14:43:31.737775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "Wdiverse_test_df = pd.read_csv(\"diverse_test.csv\")\n",
    "Wdiverse_test_df.info()"
   ],
   "id": "2a8a1fff4ceeb532",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1532 entries, 0 to 1531\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   code_snip  1532 non-null   object\n",
      " 1   output     1532 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.1+ KB\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T14:09:54.611670Z",
     "start_time": "2025-05-30T14:04:11.270464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './codebert_trainFUNC_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "Wdiverse_test_df = Wdiverse_test_df.rename(columns={\"code_snip\": \"func\", \"output\": \"label\"})\n",
    "Wdiverse_test_df = Wdiverse_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "Wdiverse_test_ds = Dataset.from_pandas(Wdiverse_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "Wdiverse_test_tok = Wdiverse_test_ds.map(tokenize_fn, batched=True)\n",
    "Wdiverse_test_tok = Wdiverse_test_tok.rename_column(\"label\", \"labels\")\n",
    "Wdiverse_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=Wdiverse_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on diverse without Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "2f6378fe1a117fae",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1532/1532 [00:00<00:00, 4419.86 examples/s]\n",
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_10548\\1092367817.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [192/192 05:39]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on diverse without Dataset ===\n",
      "loss        : 2.1496\n",
      "model_preparation_time: 0.0063\n",
      "accuracy    : 0.4830\n",
      "precision   : 0.4904\n",
      "recall      : 0.8629\n",
      "specificity : 0.1031\n",
      "fpr         : 0.8969\n",
      "f1          : 0.6254\n",
      "mcc         : -0.0522\n",
      "kappa       : -0.0339\n",
      "mse         : 0.5170\n",
      "mae         : 0.5170\n",
      "auc         : 0.4458\n",
      "runtime     : 342.7031\n",
      "samples_per_second: 4.4700\n",
      "steps_per_second: 0.5600\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Djuliet- without fol CB",
   "id": "a87d86bd0e7c5b1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T14:13:17.041942Z",
     "start_time": "2025-05-30T14:13:16.978342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "Wjuliet_test_df = pd.read_csv(\"djuliet_test.csv\")\n",
    "Wjuliet_test_df.info()"
   ],
   "id": "ce03db3bdd1f9bf7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3152 entries, 0 to 3151\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   code_snip  3152 non-null   object\n",
      " 1   output     3152 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 49.4+ KB\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T14:26:52.292830Z",
     "start_time": "2025-05-30T14:14:22.644493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './codebert_trainFUNC_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "Wjuliet_test_df = Wjuliet_test_df.rename(columns={\"code_snip\": \"func\", \"output\": \"label\"})\n",
    "Wjuliet_test_df = Wjuliet_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "Wjuliet_test_ds = Dataset.from_pandas(Wjuliet_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "Wjuliet_test_tok = Wjuliet_test_ds.map(tokenize_fn, batched=True)\n",
    "Wjuliet_test_tok = Wjuliet_test_tok.rename_column(\"label\", \"labels\")\n",
    "Wjuliet_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=Wjuliet_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on Juliet without Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "88ae965a04eae0f9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3152/3152 [00:00<00:00, 3815.67 examples/s]\n",
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_10548\\3416590620.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='394' max='394' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [394/394 12:25]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on Juliet without Dataset ===\n",
      "loss        : 2.2148\n",
      "model_preparation_time: 0.0000\n",
      "accuracy    : 0.4835\n",
      "precision   : 0.4910\n",
      "recall      : 0.9029\n",
      "specificity : 0.0641\n",
      "fpr         : 0.9359\n",
      "f1          : 0.6361\n",
      "mcc         : -0.0606\n",
      "kappa       : -0.0330\n",
      "mse         : 0.5165\n",
      "mae         : 0.5165\n",
      "auc         : 0.3482\n",
      "runtime     : 747.9077\n",
      "samples_per_second: 4.2140\n",
      "steps_per_second: 0.5270\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Mixvul without CB",
   "id": "1424addfcf266600"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T14:27:08.875463Z",
     "start_time": "2025-05-30T14:27:08.798007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "wmix_test_df = pd.read_csv(\"mix_test_vultest.csv\")\n",
    "wmix_test_df.info()"
   ],
   "id": "b94931af390d3565",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2864 entries, 0 to 2863\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  2864 non-null   object\n",
      " 1   input        2864 non-null   object\n",
      " 2   output       2864 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 67.2+ KB\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T14:41:55.203780Z",
     "start_time": "2025-05-30T14:27:12.259391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './codebert_trainFUNC_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "wmix_test_df = wmix_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "wmix_test_df = wmix_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "wmix_test_ds = Dataset.from_pandas(wmix_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "wmix_test_tok = wmix_test_ds.map(tokenize_fn, batched=True)\n",
    "wmix_test_tok = wmix_test_tok.rename_column(\"label\", \"labels\")\n",
    "wmix_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=wmix_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on mix without Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "7ea73a38a9948754",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2864/2864 [00:00<00:00, 4754.67 examples/s]\n",
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_10548\\3135542603.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='358' max='358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [358/358 14:40]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on mix without Dataset ===\n",
      "loss        : 1.9538\n",
      "model_preparation_time: 0.0000\n",
      "accuracy    : 0.4962\n",
      "precision   : 0.4976\n",
      "recall      : 0.8108\n",
      "specificity : 0.1816\n",
      "fpr         : 0.8184\n",
      "f1          : 0.6167\n",
      "mcc         : -0.0099\n",
      "kappa       : -0.0077\n",
      "mse         : 0.5038\n",
      "mae         : 0.5038\n",
      "auc         : 0.5010\n",
      "runtime     : 881.9893\n",
      "samples_per_second: 3.2470\n",
      "steps_per_second: 0.4060\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "CVEfixes",
   "id": "850c27937310fa21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T04:31:39.925442Z",
     "start_time": "2025-05-31T04:31:39.642770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "Wfun_test_df = pd.read_json(\"test_512.json\")\n",
    "Wfun_test_df.info()"
   ],
   "id": "10fd8681a6911cee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4216 entries, 0 to 4215\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  4216 non-null   object\n",
      " 1   input        4216 non-null   object\n",
      " 2   output       4216 non-null   int64 \n",
      " 3   idx          4216 non-null   int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 131.9+ KB\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T15:05:56.941842Z",
     "start_time": "2025-05-30T14:45:33.658950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './codebert_trainFUNC_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "Wfun_test_df = Wfun_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "Wfun_test_df = Wfun_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "Wfun_test_ds = Dataset.from_pandas(Wfun_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "Wfun_test_tok = Wfun_test_ds.map(tokenize_fn, batched=True)\n",
    "Wfun_test_tok = Wfun_test_tok.rename_column(\"label\", \"labels\")\n",
    "Wfun_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=Wfun_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on CVEfixes Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "f2d0091331aeeb8e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4216/4216 [00:01<00:00, 3641.45 examples/s]\n",
      "C:\\Users\\user01\\AppData\\Local\\Temp\\ipykernel_10548\\1210313311.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='527' max='527' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [527/527 20:19]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on CVEfixes Dataset ===\n",
      "loss        : 2.1064\n",
      "model_preparation_time: 0.0000\n",
      "accuracy    : 0.4853\n",
      "precision   : 0.4918\n",
      "recall      : 0.8847\n",
      "specificity : 0.0859\n",
      "fpr         : 0.9141\n",
      "f1          : 0.6322\n",
      "mcc         : -0.0489\n",
      "kappa       : -0.0294\n",
      "mse         : 0.5147\n",
      "mae         : 0.5147\n",
      "auc         : 0.4586\n",
      "runtime     : 1221.4068\n",
      "samples_per_second: 3.4520\n",
      "steps_per_second: 0.4310\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#-------------------------GCB grapchcodebert for all FOL and without",
   "id": "483f656cf7541c5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T16:00:14.779453Z",
     "start_time": "2025-07-18T16:00:10.238040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv('train_fol.csv')\n",
    "test_df = pd.read_csv('test_fol.csv')\n",
    "# ====== STEP 1: Data Preprocessing ======\n",
    "train_df = train_df.rename(columns={\n",
    "    \"func_cleaned\": \"func\",\n",
    "    \"target\": \"label\",\n",
    "    \"fol_logic\": \"fol\"\n",
    "})\n",
    "test_df = test_df.rename(columns={\n",
    "    \"func_cleaned\": \"func\",\n",
    "    \"target\": \"label\"\n",
    "})\n",
    "train_df = train_df.dropna(subset=[\"fol\", \"func\", \"label\"])\n",
    "test_df = test_df.dropna(subset=[\"func\", \"label\"])\n",
    "train_df[\"label\"] = train_df[\"label\"].astype(int)\n",
    "test_df[\"label\"] = test_df[\"label\"].astype(int)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds  = Dataset.from_pandas(test_df)\n",
    "\n",
    "# ====== STEP 2: Tokenizer Setup ======\n",
    "model_name = \"microsoft/graphcodebert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_fn_train(batch):\n",
    "    return tokenizer(\n",
    "        [\"FOL: \" + f + \" FUNC: \" + c for f, c in zip(batch[\"fol\"], batch[\"func\"])],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "def tokenize_fn_test(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "train_tok = train_ds.map(tokenize_fn_train, batched=True)\n",
    "test_tok  = test_ds.map(tokenize_fn_test, batched=True)\n",
    "\n",
    "train_tok = train_tok.rename_column('label', 'labels')\n",
    "test_tok  = test_tok.rename_column('label', 'labels')\n",
    "train_tok.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_tok.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# ====== STEP 4: Load Model ======\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.config.problem_type = \"single_label_classification\"\n",
    "model.config.id2label = {0: \"safe\", 1: \"vuln\"}\n",
    "model.config.label2id = {\"safe\": 0, \"vuln\": 1}\n",
    "\n",
    "# ====== STEP 5: Metrics ======\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "# ====== STEP 6: Training Setup ======\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "args = TrainingArguments(\n",
    "    output_dir='./graphcodebert_trainFOL_testFUNC',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=test_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "\n",
    "# ====== STEP 7: Train ======\n",
    "start_train = time.time()\n",
    "trainer.train()\n",
    "end_train = time.time()\n",
    "train_runtime = end_train - start_train\n",
    "print(f\"\\n=== Training completed in {train_runtime:.2f} seconds ({train_runtime / 60:.2f} minutes) ===\")\n",
    "\n",
    "# ====== STEP 8: Evaluate ======\n",
    "start_eval = time.time()\n",
    "metrics = trainer.evaluate()\n",
    "end_eval = time.time()\n",
    "eval_runtime = end_eval - start_eval\n",
    "print(f\"\\n=== Evaluation completed in {eval_runtime:.2f} seconds ({eval_runtime / 60:.2f} minutes) ===\")\n",
    "\n",
    "print(\"\\n=== Hold-out Metrics (Trained with FOL + FUNC, Tested with FUNC only) ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n",
    "\n",
    "print(f\"Train Time (s): {train_runtime:.2f}\")\n",
    "print(f\"Eval Time (s):  {eval_runtime:.2f}\")\n",
    "\n",
    "# ====== STEP 9: Save Model ======\n",
    "save_dir = './graphcodebert_trainFOL_testFUNC_saved'\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(f\"\\nModel and tokenizer saved to {save_dir}\")\n"
   ],
   "id": "3ae4593997dfce93",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7952/7952 [00:02<00:00, 3878.37 examples/s]\n",
      "Map: 100%|██████████| 2807/2807 [00:00<00:00, 4779.95 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 97\u001B[0m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;66;03m# ====== STEP 6: Training Setup ======\u001B[39;00m\n\u001B[0;32m     96\u001B[0m data_collator \u001B[38;5;241m=\u001B[39m DataCollatorWithPadding(tokenizer)\n\u001B[1;32m---> 97\u001B[0m args \u001B[38;5;241m=\u001B[39m \u001B[43mTrainingArguments\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     98\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./graphcodebert_trainFOL_testFUNC\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     99\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevaluation_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mepoch\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    100\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mepoch\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    101\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2e-5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    102\u001B[0m \u001B[43m    \u001B[49m\u001B[43mper_device_train_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    103\u001B[0m \u001B[43m    \u001B[49m\u001B[43mper_device_eval_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    104\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_train_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    105\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    106\u001B[0m \u001B[43m    \u001B[49m\u001B[43mload_best_model_at_end\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    107\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetric_for_best_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mf1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    108\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogging_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\n\u001B[0;32m    109\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m    111\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m    112\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m    113\u001B[0m     args\u001B[38;5;241m=\u001B[39margs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    119\u001B[0m     callbacks\u001B[38;5;241m=\u001B[39m[EarlyStoppingCallback(early_stopping_patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)]\n\u001B[0;32m    120\u001B[0m )\n\u001B[0;32m    122\u001B[0m \u001B[38;5;66;03m# ====== STEP 7: Train ======\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T02:43:55.188501Z",
     "start_time": "2025-05-31T02:43:55.158934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "# If available, print current device info\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "    print(\"Current CUDA Device:\", torch.cuda.current_device())\n",
    "else:\n",
    "    print(\"PyTorch is running on CPU.\")\n"
   ],
   "id": "ae8db8796d33a458",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: False\n",
      "PyTorch is running on CPU.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "REveal FOl GCB",
   "id": "d0df3f55f3a2c50a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T15:00:30.824845Z",
     "start_time": "2025-05-31T15:00:30.771062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "reveal_test_df = pd.read_csv(\"Reveal_vultest.csv\")\n",
    "reveal_test_df.info()"
   ],
   "id": "b81d472ded287dee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2028 entries, 0 to 2027\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  2028 non-null   object\n",
      " 1   input        2028 non-null   object\n",
      " 2   output       2028 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 47.7+ KB\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T15:00:58.954301Z",
     "start_time": "2025-05-31T15:00:43.094715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './graphcodebert_trainFOL_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "reveal_test_df = pd.read_csv(\"Reveal_vultest.csv\")\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "reveal_test_df = reveal_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "reveal_test_df = reveal_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "reveal_test_ds = Dataset.from_pandas(reveal_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "reveal_test_tok = reveal_test_ds.map(tokenize_fn, batched=True)\n",
    "reveal_test_tok = reveal_test_tok.rename_column(\"label\", \"labels\")\n",
    "reveal_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=reveal_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on Reveal Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "b3a2efaf84b802f9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2028/2028 [00:00<00:00, 3589.90 examples/s]\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='254' max='254' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [254/254 00:14]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on Reveal Dataset ===\n",
      "loss        : 0.7134\n",
      "accuracy    : 0.4956\n",
      "precision   : 0.4902\n",
      "recall      : 0.2229\n",
      "specificity : 0.7682\n",
      "fpr         : 0.2318\n",
      "f1          : 0.3064\n",
      "mcc         : -0.0106\n",
      "kappa       : -0.0089\n",
      "mse         : 0.5044\n",
      "mae         : 0.5044\n",
      "auc         : 0.5340\n",
      "runtime     : 14.2669\n",
      "samples_per_second: 142.1470\n",
      "steps_per_second: 17.8030\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "622ab154a66e0348"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T03:54:37.967824Z",
     "start_time": "2025-05-31T03:54:37.924830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "diverse_test_df = pd.read_csv(\"diverse_test.csv\")\n",
    "diverse_test_df.info()"
   ],
   "id": "5116cd290a4e9903",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1532 entries, 0 to 1531\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   code_snip  1532 non-null   object\n",
      " 1   output     1532 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.1+ KB\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T03:55:38.547538Z",
     "start_time": "2025-05-31T03:55:26.280683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './graphcodebert_trainFOL_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "diverse_test_df = diverse_test_df.rename(columns={\"code_snip\": \"func\", \"output\": \"label\"})\n",
    "diverse_test_df = diverse_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "diverse_test_ds = Dataset.from_pandas(diverse_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "diverse_test_tok = diverse_test_ds.map(tokenize_fn, batched=True)\n",
    "diverse_test_tok = diverse_test_tok.rename_column(\"label\", \"labels\")\n",
    "diverse_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=diverse_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on diverse Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "d11815db129782d7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1532/1532 [00:00<00:00, 3743.37 examples/s]\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [192/192 00:10]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on diverse Dataset ===\n",
      "loss        : 0.7014\n",
      "accuracy    : 0.5072\n",
      "precision   : 0.5256\n",
      "recall      : 0.1475\n",
      "specificity : 0.8668\n",
      "fpr         : 0.1332\n",
      "f1          : 0.2304\n",
      "mcc         : 0.0207\n",
      "kappa       : 0.0144\n",
      "mse         : 0.4928\n",
      "mae         : 0.4928\n",
      "auc         : 0.5423\n",
      "runtime     : 10.6591\n",
      "samples_per_second: 143.7270\n",
      "steps_per_second: 18.0130\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "b",
   "id": "de3faf866ba5413a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T04:49:42.103352Z",
     "start_time": "2025-05-31T04:49:42.071671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "gbig_test_df = pd.read_csv(\"big_vultest.csv\")\n",
    "gbig_test_df.info()"
   ],
   "id": "9d8dcb1923b98975",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1170 entries, 0 to 1169\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  1170 non-null   object\n",
      " 1   input        1170 non-null   object\n",
      " 2   output       1170 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 27.5+ KB\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T04:50:42.426328Z",
     "start_time": "2025-05-31T04:50:32.377192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './graphcodebert_trainFOL_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "gbig_test_df = gbig_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "gbig_test_df = gbig_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "gbig_test_ds = Dataset.from_pandas(big_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "gbig_test_tok = gbig_test_ds.map(tokenize_fn, batched=True)\n",
    "gbig_test_tok = gbig_test_tok.rename_column(\"label\", \"labels\")\n",
    "gbig_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=gbig_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on Big Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "a3a4a4d033ede66b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1170/1170 [00:00<00:00, 3516.68 examples/s]\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [147/147 00:08]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on Big Dataset ===\n",
      "loss        : 0.7199\n",
      "accuracy    : 0.4932\n",
      "precision   : 0.4500\n",
      "recall      : 0.0615\n",
      "specificity : 0.9248\n",
      "fpr         : 0.0752\n",
      "f1          : 0.1083\n",
      "mcc         : -0.0271\n",
      "kappa       : -0.0137\n",
      "mse         : 0.5068\n",
      "mae         : 0.5068\n",
      "auc         : 0.5180\n",
      "runtime     : 8.6529\n",
      "samples_per_second: 135.2140\n",
      "steps_per_second: 16.9880\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "juliet with fol gcb",
   "id": "f37684c68d38d0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T04:02:15.941899Z",
     "start_time": "2025-05-31T04:01:51.469077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './graphcodebert_trainFOL_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "juliet_test_df = juliet_test_df.rename(columns={\"code_snip\": \"func\", \"output\": \"label\"})\n",
    "juliet_test_df = juliet_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "juliet_test_ds = Dataset.from_pandas(juliet_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "juliet_test_tok = juliet_test_ds.map(tokenize_fn, batched=True)\n",
    "juliet_test_tok = juliet_test_tok.rename_column(\"label\", \"labels\")\n",
    "juliet_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=juliet_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on Juliet Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "d96ac7785669c28f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3152/3152 [00:00<00:00, 3861.42 examples/s]\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='394' max='394' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [394/394 00:22]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on Juliet Dataset ===\n",
      "loss        : 0.7092\n",
      "accuracy    : 0.5000\n",
      "precision   : 0.5000\n",
      "recall      : 0.0013\n",
      "specificity : 0.9987\n",
      "fpr         : 0.0013\n",
      "f1          : 0.0025\n",
      "mcc         : 0.0000\n",
      "kappa       : 0.0000\n",
      "mse         : 0.5000\n",
      "mae         : 0.5000\n",
      "auc         : 0.6114\n",
      "runtime     : 22.5684\n",
      "samples_per_second: 139.6640\n",
      "steps_per_second: 17.4580\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "mixvul Fol GCB",
   "id": "f6739f62600069ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T04:31:05.323122Z",
     "start_time": "2025-05-31T04:30:43.192918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './graphcodebert_trainFOL_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "mix_test_df = mix_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "mix_test_df = mix_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "mix_test_ds = Dataset.from_pandas(mix_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "mix_test_tok = mix_test_ds.map(tokenize_fn, batched=True)\n",
    "mix_test_tok = mix_test_tok.rename_column(\"label\", \"labels\")\n",
    "mix_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=mix_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on mix Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "cde12fae97b81477",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2864/2864 [00:00<00:00, 3419.73 examples/s]\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='358' max='358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [358/358 00:20]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on mix Dataset ===\n",
      "loss        : 0.7026\n",
      "accuracy    : 0.5077\n",
      "precision   : 0.5362\n",
      "recall      : 0.1138\n",
      "specificity : 0.9015\n",
      "fpr         : 0.0985\n",
      "f1          : 0.1878\n",
      "mcc         : 0.0249\n",
      "kappa       : 0.0154\n",
      "mse         : 0.4923\n",
      "mae         : 0.4923\n",
      "auc         : 0.5567\n",
      "runtime     : 20.2963\n",
      "samples_per_second: 141.1090\n",
      "steps_per_second: 17.6390\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "CVEfixes fol gcb",
   "id": "1f55f759522822f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T04:32:43.049474Z",
     "start_time": "2025-05-31T04:32:10.804864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './graphcodebert_trainFOL_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "Wfun_test_df = Wfun_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "Wfun_test_df = Wfun_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "Wfun_test_ds = Dataset.from_pandas(Wfun_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "Wfun_test_tok = Wfun_test_ds.map(tokenize_fn, batched=True)\n",
    "Wfun_test_tok = Wfun_test_tok.rename_column(\"label\", \"labels\")\n",
    "Wfun_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=Wfun_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on CVEfixes Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "a269e91e0d3b8451",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4216/4216 [00:01<00:00, 3373.91 examples/s]\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='527' max='527' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [527/527 00:29]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on CVEfixes Dataset ===\n",
      "loss        : 0.7135\n",
      "accuracy    : 0.4827\n",
      "precision   : 0.4603\n",
      "recall      : 0.2007\n",
      "specificity : 0.7647\n",
      "fpr         : 0.2353\n",
      "f1          : 0.2795\n",
      "mcc         : -0.0419\n",
      "kappa       : -0.0346\n",
      "mse         : 0.5173\n",
      "mae         : 0.5173\n",
      "auc         : 0.4679\n",
      "runtime     : 29.7075\n",
      "samples_per_second: 141.9170\n",
      "steps_per_second: 17.7400\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Diverse folGCB",
   "id": "88933ae2629d3b56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T04:34:48.050289Z",
     "start_time": "2025-05-31T04:34:34.629204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "saved_dir = './graphcodebert_trainFOL_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "diverse_test_df = diverse_test_df.rename(columns={\"code_snip\": \"func\", \"output\": \"label\"})\n",
    "diverse_test_df = diverse_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "diverse_test_ds = Dataset.from_pandas(diverse_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "diverse_test_tok = diverse_test_ds.map(tokenize_fn, batched=True)\n",
    "diverse_test_tok = diverse_test_tok.rename_column(\"label\", \"labels\")\n",
    "diverse_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=diverse_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on diverse Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "80e806cd140ac523",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1532/1532 [00:00<00:00, 3720.24 examples/s]\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [192/192 00:11]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on diverse Dataset ===\n",
      "loss        : 0.7014\n",
      "accuracy    : 0.5072\n",
      "precision   : 0.5256\n",
      "recall      : 0.1475\n",
      "specificity : 0.8668\n",
      "fpr         : 0.1332\n",
      "f1          : 0.2304\n",
      "mcc         : 0.0207\n",
      "kappa       : 0.0144\n",
      "mse         : 0.4928\n",
      "mae         : 0.4928\n",
      "auc         : 0.5423\n",
      "runtime     : 11.9285\n",
      "samples_per_second: 128.4320\n",
      "steps_per_second: 16.0960\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "=======================================================================",
   "id": "92daa60d6f3da367"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "with out fol GCB",
   "id": "a743d38daa8147d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "devign gcb without",
   "id": "9236bfd5f0700d9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T14:33:43.870290Z",
     "start_time": "2025-05-31T13:57:23.105294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== STEP 1: Load Data ======\n",
    "train_df = pd.read_csv('train_fol.csv')\n",
    "test_df = pd.read_csv('test_fol.csv')\n",
    "\n",
    "# Rename to standardized column names\n",
    "train_df = train_df.rename(columns={\n",
    "    \"func_cleaned\": \"func\",\n",
    "    \"target\": \"label\"\n",
    "})\n",
    "test_df = test_df.rename(columns={\n",
    "    \"func_cleaned\": \"func\",\n",
    "    \"target\": \"label\"\n",
    "})\n",
    "\n",
    "# Drop rows with missing values\n",
    "train_df = train_df.dropna(subset=[\"func\", \"label\"])\n",
    "test_df = test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "train_df[\"label\"] = train_df[\"label\"].astype(int)\n",
    "test_df[\"label\"] = test_df[\"label\"].astype(int)\n",
    "\n",
    "# Convert to HuggingFace Datasets\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds  = Dataset.from_pandas(test_df)\n",
    "\n",
    "# ====== STEP 2: Tokenizer Setup ======\n",
    "model_name = \"microsoft/graphcodebert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "# Apply tokenizer\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True)\n",
    "test_tok  = test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "# ====== STEP 3: Prepare datasets ======\n",
    "train_tok = train_tok.rename_column('label', 'labels')\n",
    "test_tok  = test_tok.rename_column('label', 'labels')\n",
    "train_tok.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_tok.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# ====== STEP 4: Load Model ======\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.config.problem_type = \"single_label_classification\"\n",
    "model.config.id2label = {0: \"safe\", 1: \"vuln\"}\n",
    "model.config.label2id = {\"safe\": 0, \"vuln\": 1}\n",
    "\n",
    "# ====== STEP 5: Metrics ======\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "# ====== STEP 6: Training Setup ======\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "args = TrainingArguments(\n",
    "    output_dir='./graphcodebert_trainFUNC_testFUNC',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=test_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "\n",
    "# ====== STEP 7: Train ======\n",
    "start_train = time.time()\n",
    "trainer.train()\n",
    "end_train = time.time()\n",
    "train_runtime = end_train - start_train\n",
    "print(f\"\\n=== Training completed in {train_runtime:.2f} seconds ({train_runtime / 60:.2f} minutes) ===\")\n",
    "\n",
    "# ====== STEP 8: Evaluate ======\n",
    "start_eval = time.time()\n",
    "metrics = trainer.evaluate()\n",
    "end_eval = time.time()\n",
    "eval_runtime = end_eval - start_eval\n",
    "print(f\"\\n=== Evaluation completed in {eval_runtime:.2f} seconds ({eval_runtime / 60:.2f} minutes) ===\")\n",
    "\n",
    "print(\"\\n=== Hold-out Metrics (Trained and Tested with FUNC only) ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n",
    "\n",
    "print(f\"Train Time (s): {train_runtime:.2f}\")\n",
    "print(f\"Eval Time (s):  {eval_runtime:.2f}\")\n",
    "\n",
    "# ====== STEP 9: Save Model ======\n",
    "save_dir = './graphcodebert_trainFUNC_testFUNC_saved'\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(f\"\\nModel and tokenizer saved to {save_dir}\")\n"
   ],
   "id": "f822c5b3fe64dac",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 11225/11225 [00:07<00:00, 1438.15 examples/s]\n",
      "Map: 100%|██████████| 2807/2807 [00:01<00:00, 1410.99 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7020' max='7020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7020/7020 35:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Fpr</th>\n",
       "      <th>F1</th>\n",
       "      <th>Mcc</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.646300</td>\n",
       "      <td>0.669046</td>\n",
       "      <td>0.598504</td>\n",
       "      <td>0.703196</td>\n",
       "      <td>0.126645</td>\n",
       "      <td>0.959145</td>\n",
       "      <td>0.040855</td>\n",
       "      <td>0.214634</td>\n",
       "      <td>0.158502</td>\n",
       "      <td>0.094966</td>\n",
       "      <td>0.401496</td>\n",
       "      <td>0.401496</td>\n",
       "      <td>0.643956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.555700</td>\n",
       "      <td>0.640619</td>\n",
       "      <td>0.650873</td>\n",
       "      <td>0.647132</td>\n",
       "      <td>0.426809</td>\n",
       "      <td>0.822124</td>\n",
       "      <td>0.177876</td>\n",
       "      <td>0.514371</td>\n",
       "      <td>0.273049</td>\n",
       "      <td>0.259339</td>\n",
       "      <td>0.349127</td>\n",
       "      <td>0.349127</td>\n",
       "      <td>0.710205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.527700</td>\n",
       "      <td>0.662603</td>\n",
       "      <td>0.647310</td>\n",
       "      <td>0.598776</td>\n",
       "      <td>0.563322</td>\n",
       "      <td>0.711502</td>\n",
       "      <td>0.288498</td>\n",
       "      <td>0.580508</td>\n",
       "      <td>0.277139</td>\n",
       "      <td>0.276756</td>\n",
       "      <td>0.352690</td>\n",
       "      <td>0.352690</td>\n",
       "      <td>0.719272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.378400</td>\n",
       "      <td>0.844864</td>\n",
       "      <td>0.640185</td>\n",
       "      <td>0.581876</td>\n",
       "      <td>0.601974</td>\n",
       "      <td>0.669390</td>\n",
       "      <td>0.330610</td>\n",
       "      <td>0.591754</td>\n",
       "      <td>0.270388</td>\n",
       "      <td>0.270264</td>\n",
       "      <td>0.359815</td>\n",
       "      <td>0.359815</td>\n",
       "      <td>0.711766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.302700</td>\n",
       "      <td>1.025670</td>\n",
       "      <td>0.641254</td>\n",
       "      <td>0.591747</td>\n",
       "      <td>0.554276</td>\n",
       "      <td>0.707731</td>\n",
       "      <td>0.292269</td>\n",
       "      <td>0.572399</td>\n",
       "      <td>0.264396</td>\n",
       "      <td>0.263977</td>\n",
       "      <td>0.358746</td>\n",
       "      <td>0.358746</td>\n",
       "      <td>0.710828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./graphcodebert_trainFUNC_testFUNC\\checkpoint-1404 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./graphcodebert_trainFUNC_testFUNC\\checkpoint-2808 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./graphcodebert_trainFUNC_testFUNC\\checkpoint-4212 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./graphcodebert_trainFUNC_testFUNC\\checkpoint-5616 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training completed in 2136.06 seconds (35.60 minutes) ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [351/351 00:30]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation completed in 30.47 seconds (0.51 minutes) ===\n",
      "\n",
      "=== Hold-out Metrics (Trained and Tested with FUNC only) ===\n",
      "loss        : 0.8449\n",
      "accuracy    : 0.6402\n",
      "precision   : 0.5819\n",
      "recall      : 0.6020\n",
      "specificity : 0.6694\n",
      "fpr         : 0.3306\n",
      "f1          : 0.5918\n",
      "mcc         : 0.2704\n",
      "kappa       : 0.2703\n",
      "mse         : 0.3598\n",
      "mae         : 0.3598\n",
      "auc         : 0.7118\n",
      "runtime     : 30.4603\n",
      "samples_per_second: 92.1530\n",
      "steps_per_second: 11.5230\n",
      "Train Time (s): 2136.06\n",
      "Eval Time (s):  30.47\n",
      "\n",
      "Model and tokenizer saved to ./graphcodebert_trainFUNC_testFUNC_saved\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Reveal without GCB",
   "id": "1e453d59b1482f88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T15:03:41.300878Z",
     "start_time": "2025-05-31T15:03:41.261236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "Rreveal_test_df = pd.read_csv(\"Reveal_vultest.csv\")\n",
    "Rreveal_test_df.info()"
   ],
   "id": "5e0cba826f5d7d82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2028 entries, 0 to 2027\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  2028 non-null   object\n",
      " 1   input        2028 non-null   object\n",
      " 2   output       2028 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 47.7+ KB\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T14:39:19.791199Z",
     "start_time": "2025-05-31T14:39:01.053323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "save_dir = './graphcodebert_trainFUNC_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "Wreveal_test_df = pd.read_csv(\"Reveal_vultest.csv\")\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "Wreveal_test_df = Wreveal_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "Wreveal_test_df = Wreveal_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "Wreveal_test_ds = Dataset.from_pandas(Wreveal_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "Wreveal_test_tok = Wreveal_test_ds.map(tokenize_fn, batched=True)\n",
    "Wreveal_test_tok = Wreveal_test_tok.rename_column(\"label\", \"labels\")\n",
    "Wreveal_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=Wreveal_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on Reveal without Fol Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "d4168915f449d9e3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2028/2028 [00:00<00:00, 3069.25 examples/s]\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='254' max='254' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [254/254 00:16]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on Reveal without Fol Dataset ===\n",
      "loss        : 0.7134\n",
      "accuracy    : 0.4956\n",
      "precision   : 0.4902\n",
      "recall      : 0.2229\n",
      "specificity : 0.7682\n",
      "fpr         : 0.2318\n",
      "f1          : 0.3064\n",
      "mcc         : -0.0106\n",
      "kappa       : -0.0089\n",
      "mse         : 0.5044\n",
      "mae         : 0.5044\n",
      "auc         : 0.5340\n",
      "runtime     : 16.4672\n",
      "samples_per_second: 123.1540\n",
      "steps_per_second: 15.4250\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "bigVul test gcb without",
   "id": "2e98bfae9e5b5b93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T14:43:50.879363Z",
     "start_time": "2025-05-31T14:43:40.571050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "save_dir = './graphcodebert_trainFUNC_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "Wbig_test_df = Wbig_test_df.rename(columns={\"input\": \"func\", \"output\": \"label\"})\n",
    "Wbig_test_df = Wbig_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "Wbig_test_ds = Dataset.from_pandas(Wbig_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "Wbig_test_tok = Wbig_test_ds.map(tokenize_fn, batched=True)\n",
    "Wbig_test_tok = Wbig_test_tok.rename_column(\"label\", \"labels\")\n",
    "Wbig_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=Wbig_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on Big without Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "cfded31aa9ea59c2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1170/1170 [00:00<00:00, 3523.41 examples/s]\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [147/147 00:08]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on Big without Dataset ===\n",
      "loss        : 0.7199\n",
      "accuracy    : 0.4932\n",
      "precision   : 0.4500\n",
      "recall      : 0.0615\n",
      "specificity : 0.9248\n",
      "fpr         : 0.0752\n",
      "f1          : 0.1083\n",
      "mcc         : -0.0271\n",
      "kappa       : -0.0137\n",
      "mse         : 0.5068\n",
      "mae         : 0.5068\n",
      "auc         : 0.5180\n",
      "runtime     : 8.8958\n",
      "samples_per_second: 131.5230\n",
      "steps_per_second: 16.5250\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Diverse withour Fol GCB",
   "id": "d9f842ee60f862b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T14:57:26.987815Z",
     "start_time": "2025-05-31T14:57:14.915828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, matthews_corrcoef,\n",
    "    cohen_kappa_score, mean_squared_error,\n",
    "    mean_absolute_error, roc_auc_score\n",
    ")\n",
    "\n",
    "# ====== LOAD SAVED MODEL ======\n",
    "save_dir = './graphcodebert_trainFUNC_testFUNC_saved'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(saved_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_dir)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ====== LOAD AND PROCESS NEW TEST DATA ======\n",
    "# Replace with your actual new test DataFrame\n",
    "\n",
    "\n",
    "# Standardize column names and drop missing values\n",
    "Wdiverse_test_df = Wdiverse_test_df.rename(columns={\"code_snip\": \"func\", \"output\": \"label\"})\n",
    "Wdiverse_test_df = Wdiverse_test_df.dropna(subset=[\"func\", \"label\"])\n",
    "\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "Wdiverse_test_ds = Dataset.from_pandas(Wdiverse_test_df)\n",
    "\n",
    "# Tokenization (FUNC only, no FOL)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        [\"FUNC: \" + c for c in batch[\"func\"]],\n",
    "        truncation=True, padding='max_length', max_length=256\n",
    "    )\n",
    "\n",
    "Wdiverse_test_tok = Wdiverse_test_ds.map(tokenize_fn, batched=True)\n",
    "Wdiverse_test_tok = Wdiverse_test_tok.rename_column(\"label\", \"labels\")\n",
    "Wdiverse_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"accuracy\":    accuracy_score(labels, preds),\n",
    "        \"precision\":   precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\":      recall_score(labels, preds, zero_division=0),\n",
    "        \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        \"fpr\":         fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"f1\":          f1_score(labels, preds, zero_division=0),\n",
    "        \"mcc\":         matthews_corrcoef(labels, preds),\n",
    "        \"kappa\":       cohen_kappa_score(labels, preds),\n",
    "        \"mse\":         mean_squared_error(labels, preds),\n",
    "        \"mae\":         mean_absolute_error(labels, preds),\n",
    "        \"auc\":         auc\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=Wdiverse_test_tok)\n",
    "\n",
    "# ====== PRINT METRICS ======\n",
    "print(\"\\n=== Evaluation on diverse without Dataset ===\")\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:]:<12}: {value:.4f}\")\n"
   ],
   "id": "2f825569de4dbae8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1532/1532 [00:00<00:00, 4100.01 examples/s]\n",
      "C:\\Users\\user01\\PycharmProjects\\fol\\.venv\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [192/192 00:10]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation on diverse without Dataset ===\n",
      "loss        : 0.7014\n",
      "accuracy    : 0.5072\n",
      "precision   : 0.5256\n",
      "recall      : 0.1475\n",
      "specificity : 0.8668\n",
      "fpr         : 0.1332\n",
      "f1          : 0.2304\n",
      "mcc         : 0.0207\n",
      "kappa       : 0.0144\n",
      "mse         : 0.4928\n",
      "mae         : 0.4928\n",
      "auc         : 0.5423\n",
      "runtime     : 10.8439\n",
      "samples_per_second: 141.2770\n",
      "steps_per_second: 17.7060\n"
     ]
    }
   ],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
